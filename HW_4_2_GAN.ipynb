{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f4ec1dc-dfcf-4e38-9e9b-7299a3506200",
   "metadata": {},
   "source": [
    "# Understanding GANs\n",
    "\n",
    "Imagine walking into an art studio where something fascinating is happening: there's an artist and an art critic engaged in an unusual competition. The artist is trying to create paintings that look exactly like works from a famous gallery, while the critic's job is to spot the difference between real gallery paintings and the artist's creations. This back-and-forth between artist and critic perfectly captures how GANs (Generative Adversarial Networks) work.\n",
    "\n",
    "## Core Intuition \n",
    "\n",
    "In our Swiss Roll example, we can think of it this way: The artist (Generator) is trying to create points that form the distinctive spiral pattern of a Swiss Roll, while the critic (Discriminator) needs to distinguish between points from the real Swiss Roll distribution and the artist's generated points. Through this competition, both networks improve – the artist gets better at creating realistic points, and the critic becomes more discerning.\n",
    "\n",
    "## The Mathematical Dance \n",
    "\n",
    "This artistic competition translates into a mathematical game. The Generator (G) and Discriminator (D) play what we call a minimax game, represented by this equation:\n",
    "\n",
    "$$\n",
    "\\min_G \\max_D \\mathbb{E}_{x}[\\log D(x)] + \\mathbb{E}_{z}[\\log(1 - D(G(z)))]\n",
    "$$\n",
    "\n",
    "Breaking this down in simple terms:\n",
    "\n",
    "- D(x) is the critic's judgment of real data (should be close to 1)\n",
    "- G(z) is the artist creating new data from random noise z\n",
    "- D(G(z)) is the critic's judgment of generated data (should be close to 0)\n",
    "- The Generator tries to minimize this difference\n",
    "- The Discriminator tries to maximize it\n",
    "\n",
    "<!-- \n",
    "\n",
    "Unlike other generative models we've seen:\n",
    "\n",
    "- VAEs work like photographers who compress and reconstruct images\n",
    "- Flow Matching works like a choreographer planning smooth movements\n",
    "- GANs work like artists learning through feedback and competition\n",
    "\n",
    "This competitive approach often leads to:\n",
    "\n",
    "1. Sharper, more realistic outputs\n",
    "2. No need for explicit probability calculations\n",
    "3. Direct optimization of generation quality -->\n",
    "\n",
    "### The Latent Space (potential homework? to explore)\n",
    "\n",
    "The noise input to our Generator isn't just random numbers – it's a structured space where:\n",
    "\n",
    "- Similar points generate similar outputs\n",
    "- We can interpolate between points\n",
    "- Different dimensions might control different features\n",
    "\n",
    "## Key Papers \n",
    "For a deeper dive into GANs, consider reading:\n",
    "\n",
    "* \"Generative Adversarial Networks\" by Goodfellow et al. (2014)\n",
    "* \"Wasserstein GAN\" by Arjovsky et al. (2017)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57067a6-7523-434f-ab8c-9f2e2af34565",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate Swiss Roll data (same as before)\n",
    "def generate_swiss_roll(n_samples=1000):\n",
    "    \"\"\"\n",
    "    Generate 2D Swiss Roll data points\n",
    "    Args:\n",
    "        n_samples: Number of points to generate\n",
    "    Returns:\n",
    "        torch.Tensor of shape (n_samples, 2)\n",
    "    \"\"\"\n",
    "    t = 1.5 * np.pi * (1 + 2 * torch.rand(n_samples))\n",
    "    x = t * torch.cos(t)\n",
    "    y = t * torch.sin(t)\n",
    "    data = torch.stack([x, y], dim=1) / 15.0  # Scale down the data\n",
    "    return data\n",
    "\n",
    "data = generate_swiss_roll(n_samples=10000)\n",
    "plt.scatter(data[:, 0], data[:, 1], alpha=0.5, s=10)\n",
    "plt.title('Sythetic Swiss Roll Distribution')\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    \"\"\"\n",
    "    Generator network that transforms random noise into 2D points \n",
    "    to mimic the Swiss Roll distribution\n",
    "    \"\"\"\n",
    "    def __init__(self, latent_dim=2, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim, 2),  # Output 2D coordinates\n",
    "            nn.Tanh()  # Bound outputs to [-1, 1]\n",
    "        )\n",
    "    \n",
    "    def forward(self, z):\n",
    "        return self.net(z)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    \"\"\"\n",
    "    Discriminator network that tries to distinguish real Swiss Roll points \n",
    "    from generated ones\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(2, hidden_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Sigmoid()  # Output probability between 0 and 1\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "def train_gan(n_steps=10000, batch_size=128, lr=2e-4, latent_dim=2):\n",
    "    \"\"\"\n",
    "    Train the GAN on Swiss Roll data\n",
    "    Args:\n",
    "        n_steps: Number of training steps\n",
    "        batch_size: Batch size for training\n",
    "        lr: Learning rate\n",
    "        latent_dim: Dimension of noise input to generator\n",
    "    Returns:\n",
    "        Trained generator and discriminator models, training losses, device\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Initialize networks and optimizers\n",
    "    generator = Generator(latent_dim=latent_dim).to(device)\n",
    "    discriminator = Discriminator().to(device)\n",
    "    \n",
    "    # Use Adam optimizer with beta parameters recommended for GANs\n",
    "    g_optimizer = torch.optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "    d_optimizer = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "    \n",
    "    criterion = nn.BCELoss()\n",
    "    \n",
    "    # Training loop\n",
    "    g_losses, d_losses = [], []\n",
    "    pbar = tqdm(range(n_steps), desc=\"Training GAN\")\n",
    "    \n",
    "    for step in pbar:\n",
    "        # Train Discriminator\n",
    "        for _ in range(1):  # Can adjust D/G training ratio\n",
    "            # Real data\n",
    "            real_data = generate_swiss_roll(batch_size).to(device)\n",
    "            real_labels = torch.ones(batch_size, 1).to(device)\n",
    "            \n",
    "            # Generated data\n",
    "            z = torch.randn(batch_size, latent_dim).to(device)\n",
    "            fake_data = generator(z).detach()\n",
    "            fake_labels = torch.zeros(batch_size, 1).to(device)\n",
    "            \n",
    "            # Train on real and fake data\n",
    "            d_optimizer.zero_grad()\n",
    "            d_real_loss = criterion(discriminator(real_data), real_labels)\n",
    "            d_fake_loss = criterion(discriminator(fake_data), fake_labels)\n",
    "            d_loss = d_real_loss + d_fake_loss\n",
    "            d_loss.backward()\n",
    "            d_optimizer.step()\n",
    "        \n",
    "        # Train Generator\n",
    "        for _ in range(1):\n",
    "            z = torch.randn(batch_size, latent_dim).to(device)\n",
    "            g_optimizer.zero_grad()\n",
    "            fake_data = generator(z)\n",
    "            g_loss = criterion(discriminator(fake_data), real_labels)  # Try to fool D\n",
    "            g_loss.backward()\n",
    "            g_optimizer.step()\n",
    "        \n",
    "        # Record losses\n",
    "        g_losses.append(g_loss.item())\n",
    "        d_losses.append(d_loss.item())\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            pbar.set_postfix({'G_Loss': g_loss.item(), 'D_Loss': d_loss.item()})\n",
    "    \n",
    "    return generator, discriminator, g_losses, d_losses, device\n",
    "\n",
    "\n",
    "# Train the model\n",
    "generator, discriminator, g_losses, d_losses, device = train_gan()\n",
    "\n",
    "# Plot training losses\n",
    "plt.plot(g_losses, label='Generator')\n",
    "plt.plot(d_losses, label='Discriminator')\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "def visualize_gan_samples(generator, n_samples=1000):\n",
    "    \"\"\"\n",
    "    Visualize samples from the trained generator alongside real data\n",
    "    Args:\n",
    "        generator: Trained generator model\n",
    "        n_samples: Number of points to generate\n",
    "    \"\"\"\n",
    "    device = next(generator.parameters()).device\n",
    "    \n",
    "    # Generate samples\n",
    "    with torch.no_grad():\n",
    "        z = torch.randn(n_samples, 2).to(device)\n",
    "        fake_data = generator(z).cpu().numpy()\n",
    "    \n",
    "    # Get real data for comparison\n",
    "    real_data = generate_swiss_roll(n_samples).numpy()\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.scatter(real_data[:, 0], real_data[:, 1], \n",
    "                c='blue', alpha=0.5, label='Real')\n",
    "    plt.title('Real Swiss Roll')\n",
    "    plt.legend()\n",
    "    plt.axis('equal')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.scatter(fake_data[:, 0], fake_data[:, 1], \n",
    "                c='red', alpha=0.5, label='Generated')\n",
    "    plt.title('GAN Generated')\n",
    "    plt.legend()\n",
    "    plt.axis('equal')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize results\n",
    "visualize_gan_samples(generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f96da1e-022f-40ac-bb5c-06ec14c75bdf",
   "metadata": {},
   "source": [
    "\n",
    "# Generative Adversarial Network (GAN) for Swiss Roll Data\n",
    "\n",
    "In this notebook you will implement—in part—the training loop for a GAN on a 2D Swiss Roll dataset.\n",
    "\n",
    "**Overview:**\n",
    "\n",
    "- **Generator:**  \n",
    "  Transforms random noise into 2D points that mimic the Swiss Roll data.\n",
    "\n",
    "- **Discriminator:**  \n",
    "  Distinguishes real Swiss Roll points from those generated by the generator.\n",
    "\n",
    "- **Training Objectives:**  \n",
    "  - **Discriminator:** Maximize the probability of assigning correct labels to real and fake samples.\n",
    "  - **Generator:** Fool the discriminator by generating samples that are labeled as real.\n",
    "\n",
    "**Your Task:**  \n",
    "Review and complete the GAN training loop. In particular, you will work on the part where:\n",
    "  - Real and fake labels are created.\n",
    "  - The discriminator loss is computed on real data (with true labels) and on fake data (with fake labels).\n",
    "  - The generator loss is computed by trying to convince the discriminator that generated samples are real.\n",
    "\n",
    "Study the provided hints and inline comments carefully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e885b6bf-be2a-4e54-aab3-8adae0d007ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim  # Import the optimizer module\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set random seed for reproducibility.\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f303553-b764-419d-b83c-8ac69b8638d1",
   "metadata": {},
   "source": [
    "## Data Generation\n",
    "\n",
    "We define a function to generate 2D Swiss Roll data points. This will serve as our real data distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d90a4f-5598-4fa2-b1a0-397c407144ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_swiss_roll(n_samples=1000):\n",
    "    \"\"\"\n",
    "    Generate 2D Swiss Roll data points.\n",
    "    \n",
    "    Args:\n",
    "        n_samples: Number of points to generate.\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor of shape (n_samples, 2)\n",
    "    \"\"\"\n",
    "    t = 1.5 * np.pi * (1 + 2 * torch.rand(n_samples))\n",
    "    x = t * torch.cos(t)\n",
    "    y = t * torch.sin(t)\n",
    "    data = torch.stack([x, y], dim=1) / 15.0  # Scale down the data\n",
    "    return data\n",
    "\n",
    "# Visualize some Swiss Roll data.\n",
    "data = generate_swiss_roll(n_samples=10000)\n",
    "plt.scatter(data[:, 0].detach().numpy(), data[:, 1].detach().numpy(), alpha=0.5, s=10)\n",
    "plt.title('Synthetic Swiss Roll Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8d665f-0600-4da3-925c-90edb0905a2b",
   "metadata": {},
   "source": [
    "## Network Architectures\n",
    "\n",
    "Below are the Generator and Discriminator network definitions.\n",
    "\n",
    "- **Generator:**  \n",
    "  Takes a latent noise vector and outputs a 2D sample.  \n",
    "  The final Tanh activation bounds outputs to $[-1,1]$.\n",
    "\n",
    "- **Discriminator:**  \n",
    "  Takes a 2D input and outputs a scalar probability (after Sigmoid) indicating whether the input is real.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e05058-a6cb-45aa-8b1f-a04f8e673a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"\"\"\n",
    "    Generator network that transforms random noise into 2D points \n",
    "    to mimic the Swiss Roll distribution.\n",
    "    \"\"\"\n",
    "    def __init__(self, latent_dim=2, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim, 2),  # Output 2D coordinates\n",
    "            nn.Tanh()  # Bound outputs to [-1, 1]\n",
    "        )\n",
    "    \n",
    "    def forward(self, z):\n",
    "        return self.net(z)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    \"\"\"\n",
    "    Discriminator network that tries to distinguish real Swiss Roll points \n",
    "    from generated ones.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(2, hidden_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Sigmoid()  # Output probability between 0 and 1.\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba74b145-75fc-422f-ac88-a1f0f4f1c938",
   "metadata": {},
   "source": [
    "## GAN Training Loop\n",
    "\n",
    "In this cell you'll write the training loop for the GAN. Pay close attention to the following steps:\n",
    "\n",
    "1. **Discriminator Training:**\n",
    "   - **Real Data:**  \n",
    "     Sample a batch of real Swiss Roll points and assign them the label 1.\n",
    "   - **Fake Data:**  \n",
    "     Sample random noise, generate fake points with the Generator, and assign them the label 0.\n",
    "   - **Discriminator Loss:**  \n",
    "     Compute the Binary Cross Entropy (BCE) loss for both real and fake samples and sum them.\n",
    "     \n",
    "2. **Generator Training:**\n",
    "   - Generate fake data from random noise.\n",
    "   - Compute the generator loss by passing the fake samples through the discriminator and  \n",
    "     comparing them to the true label (1) because the generator wants to fool the discriminator.\n",
    "     \n",
    "**TODO:**  \n",
    "Implement (or study and understand) the loss calculations for both generator and discriminator.  \n",
    "Check that:\n",
    "- The discriminator uses correct true labels for real data (`1`) and fake data (`0`).\n",
    "- The generator is trained with fake data but with the target label set to `1`.\n",
    "  \n",
    "HINTS are provided in the inline comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09b8595-bdcb-400a-b26f-681bf22964dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gan(n_steps=10000, batch_size=128, lr=2e-4, latent_dim=2):\n",
    "    \"\"\"\n",
    "    Train the GAN on Swiss Roll data.\n",
    "\n",
    "    Args:\n",
    "        n_steps (int): Number of training steps.\n",
    "        batch_size (int): Batch size for training.\n",
    "        lr (float): Learning rate.\n",
    "        latent_dim (int): Dimension of the noise input for the generator.\n",
    "    \n",
    "    Returns:\n",
    "        generator, discriminator, g_losses, d_losses, device\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Initialize networks.\n",
    "    generator = Generator(latent_dim=latent_dim).to(device)\n",
    "    discriminator = Discriminator().to(device)\n",
    "    \n",
    "    # Set up optimizers with recommended beta parameters for GAN training.\n",
    "    g_optimizer = torch.optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "    d_optimizer = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "    \n",
    "    # Define the binary cross-entropy loss function.\n",
    "    criterion = nn.BCELoss()\n",
    "    \n",
    "    # Containers for losses.\n",
    "    g_losses, d_losses = [], []\n",
    "    \n",
    "    from tqdm import tqdm  # Ensures that tqdm is imported if not already.\n",
    "    pbar = tqdm(range(n_steps), desc=\"Training GAN\")\n",
    "    \n",
    "    for step in pbar:\n",
    "        # ------------------ Train Discriminator ------------------\n",
    "        for _ in range(1):\n",
    "            # Sample a batch of real data.\n",
    "            real_data = generate_swiss_roll(batch_size).to(device)\n",
    "            real_labels = torch.ones(batch_size, 1, device=device)  # Real labels are 1.\n",
    "\n",
    "            # Sample noise from the latent distribution and generate fake data.\n",
    "            z = torch.randn(batch_size, latent_dim, device=device)\n",
    "            fake_data = generator(z).detach()  # Detach to avoid backprop into generator.\n",
    "            fake_labels = torch.zeros(batch_size, 1, device=device)  # Fake labels are 0.\n",
    "            \n",
    "            # Zero the gradients for discriminator.\n",
    "            d_optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass through the discriminator.\n",
    "            real_preds = discriminator(real_data)\n",
    "            fake_preds = discriminator(fake_data)\n",
    "            \n",
    "            # TODO: Compute discriminator loss:\n",
    "            #   1. Use your chosen criterion on (real_preds, real_labels) to calculate the loss for real samples.\n",
    "            #   2. Do the same for fake samples with (fake_preds, fake_labels).\n",
    "            #   3. Sum or average these two losses appropriately.\n",
    "            # Replace the following line with your implementation.\n",
    "            raise NotImplementedError(\"Compute discriminator loss using real_preds and fake_preds\")\n",
    "            # d_real_loss = <YOUR CODE HERE>\n",
    "            # d_fake_loss = <YOUR CODE HERE>\n",
    "            # d_loss = d_real_loss + d_fake_loss\n",
    "            \n",
    "            d_loss.backward()\n",
    "            d_optimizer.step()\n",
    "        \n",
    "        # ------------------ Train Generator ------------------\n",
    "        for _ in range(1):\n",
    "            # Sample noise to generate fake data.\n",
    "            z = torch.randn(batch_size, latent_dim, device=device)\n",
    "            g_optimizer.zero_grad()\n",
    "            fake_data = generator(z)\n",
    "            \n",
    "            # TODO: Compute the generator loss:\n",
    "            #   1. Pass fake_data through the discriminator.\n",
    "            #   2. Compare the output with real labels (ones) because the generator's goal is to fool the discriminator.\n",
    "            # Replace the following line with your implementation.\n",
    "            raise NotImplementedError(\"Compute generator loss by comparing discriminator(fake_data) with real labels\")\n",
    "            # g_loss = <YOUR CODE HERE>\n",
    "            \n",
    "            g_loss.backward()\n",
    "            g_optimizer.step()\n",
    "        \n",
    "        # Record losses.\n",
    "        g_losses.append(g_loss.item())\n",
    "        d_losses.append(d_loss.item())\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            pbar.set_postfix({'G_Loss': g_loss.item(), 'D_Loss': d_loss.item()})\n",
    "    \n",
    "    return generator, discriminator, g_losses, d_losses, device\n",
    "    \n",
    "# Train the GAN.\n",
    "generator, discriminator, g_losses, d_losses, device = train_gan()\n",
    "\n",
    "# %% [code]\n",
    "plt.plot(g_losses, label='Generator Loss')\n",
    "plt.plot(d_losses, label='Discriminator Loss')\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('GAN Training Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419ab24e-c8fd-442c-9037-f086e8fc7c66",
   "metadata": {},
   "source": [
    "## Visualizing GAN Results\n",
    "\n",
    "Finally, we visualize samples generated from the trained generator alongside real Swiss Roll data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbcbb94-0728-4259-8f1f-4a2507f7228c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_gan_samples(generator, n_samples=1000):\n",
    "    \"\"\"\n",
    "    Visualize samples from the generator along with real data.\n",
    "    \n",
    "    Args:\n",
    "        generator: Trained generator model.\n",
    "        n_samples: Number of samples to generate.\n",
    "    \"\"\"\n",
    "    device = next(generator.parameters()).device\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Generate fake samples.\n",
    "        z = torch.randn(n_samples, 2, device=device)\n",
    "        fake_data = generator(z).cpu().numpy()\n",
    "    \n",
    "    # Get real data for comparison.\n",
    "    real_data = generate_swiss_roll(n_samples).numpy()\n",
    "    \n",
    "    # Create side-by-side plots.\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.scatter(real_data[:, 0], real_data[:, 1], c='blue', alpha=0.5, label='Real')\n",
    "    plt.title('Real Swiss Roll Data')\n",
    "    plt.legend()\n",
    "    plt.axis('equal')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.scatter(fake_data[:, 0], fake_data[:, 1], c='red', alpha=0.5, label='Generated')\n",
    "    plt.title('GAN Generated Data')\n",
    "    plt.legend()\n",
    "    plt.axis('equal')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize the generated results.\n",
    "visualize_gan_samples(generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d7c009-664d-4496-a265-1d5b3e458f72",
   "metadata": {},
   "source": [
    "# Extra Resource for Reference\n",
    "## Understanding GANs\n",
    "\n",
    "Imagine walking into an art studio where something fascinating is happening: there's an artist and an art critic engaged in an unusual competition. The artist is trying to create paintings that look exactly like works from a famous gallery, while the critic's job is to spot the difference between real gallery paintings and the artist's creations. This back-and-forth between artist and critic perfectly captures how GANs (Generative Adversarial Networks) work.\n",
    "\n",
    "## Core Intuition \n",
    "\n",
    "In our Swiss Roll example, we can think of it this way: The artist (Generator) is trying to create points that form the distinctive spiral pattern of a Swiss Roll, while the critic (Discriminator) needs to distinguish between points from the real Swiss Roll distribution and the artist's generated points. Through this competition, both networks improve – the artist gets better at creating realistic points, and the critic becomes more discerning.\n",
    "\n",
    "## The Mathematical Dance \n",
    "\n",
    "This artistic competition translates into a mathematical game. The Generator (G) and Discriminator (D) play what we call a minimax game, represented by this equation:\n",
    "\n",
    "$$\n",
    "\\min_G \\max_D \\mathbb{E}_{x}[\\log D(x)] + \\mathbb{E}_{z}[\\log(1 - D(G(z)))]\n",
    "$$\n",
    "\n",
    "Breaking this down in simple terms:\n",
    "\n",
    "- D(x) is the critic's judgment of real data (should be close to 1)\n",
    "- G(z) is the artist creating new data from random noise z\n",
    "- D(G(z)) is the critic's judgment of generated data (should be close to 0)\n",
    "- The Generator tries to minimize this difference\n",
    "- The Discriminator tries to maximize it\n",
    "\n",
    "<!-- \n",
    "\n",
    "Unlike other generative models we've seen:\n",
    "\n",
    "- VAEs work like photographers who compress and reconstruct images\n",
    "- Flow Matching works like a choreographer planning smooth movements\n",
    "- GANs work like artists learning through feedback and competition\n",
    "\n",
    "This competitive approach often leads to:\n",
    "\n",
    "1. Sharper, more realistic outputs\n",
    "2. No need for explicit probability calculations\n",
    "3. Direct optimization of generation quality -->\n",
    "\n",
    "### The Latent Space (potential homework? to explore)\n",
    "\n",
    "The noise input to our Generator isn't just random numbers – it's a structured space where:\n",
    "\n",
    "- Similar points generate similar outputs\n",
    "- We can interpolate between points\n",
    "- Different dimensions might control different features\n",
    "\n",
    "## Key Papers \n",
    "For a deeper dive into GANs, consider reading:\n",
    "\n",
    "* \"Generative Adversarial Networks\" by Goodfellow et al. (2014)\n",
    "* \"Wasserstein GAN\" by Arjovsky et al. (2017)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
