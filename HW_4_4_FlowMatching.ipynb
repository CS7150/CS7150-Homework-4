{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16f1a296-23a6-4587-a084-22f8563ae844",
   "metadata": {},
   "source": [
    "\n",
    "# Flow Matching for Swiss Roll Data\n",
    "\n",
    "In this notebook, we apply the flow matching methodology to learn a transport map between a simple base distribution and a target Swiss Roll distribution.\n",
    "\n",
    "**Key Ideas:**\n",
    "- **Training Objective:**  \n",
    "  Given a pair of samples $x_0 \\sim \\mathcal{N}(0, I)$ (base) and $x_1 \\sim p_{\\text{data}}$ (target), we compute a linear interpolation  \n",
    "$$\n",
    "  x_t = (1-t)\\,x_0 + t\\,x_1,\\quad t \\in [0,1],\n",
    "$$\n",
    "  and define the ground-truth velocity field as:\n",
    "$$\n",
    "  v_{\\text{target}} = x_1 - x_0.\n",
    "$$\n",
    "  The network takes as input $(x_t,t)$ and is trained to predict $v(x_t,t) \\approx v_{\\text{target}}$.\n",
    "\n",
    "- **Sampling:**  \n",
    "  Once trained, we can generate samples from the target distribution by starting from a sample \\(x_0 \\sim \\mathcal{N}(0,I)\\) and integrating the ODE:\n",
    "  $$\n",
    "  \\frac{dx}{dt} = v(x,t),\n",
    "  $$\n",
    "  from $t=0$ to $t=1$.\n",
    "\n",
    "**Your Tasks:**\n",
    "- Study all code cells and comments.\n",
    "- Experiment with the integration (try improving the Euler solver).\n",
    "- Compare with the corresponding diffusion model (which uses a noise‚Äêremoval objective rather than direct flow matching).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6e20bc-bbdb-480e-81ca-1f9e99584b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim  \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98859031-74f5-447e-a5e4-1f08b625f1ca",
   "metadata": {},
   "source": [
    "\n",
    "## Data Generation\n",
    "\n",
    "We define two helper functions:\n",
    "1. `generate_swiss_roll`: Generates target Swiss Roll data (in $\\mathbb{R}^2$).\n",
    "2. `sample_gaussian`: Samples from the base Gaussian distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d1473f-e621-485b-8a49-b5d01c64cfa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_swiss_roll(n_samples=1000, device='cpu'):\n",
    "    \"\"\"\n",
    "    Generate 2D Swiss Roll data points.\n",
    "    \n",
    "    Args:\n",
    "        n_samples: Number of points.\n",
    "        device: Device to place the tensor.\n",
    "        \n",
    "    Returns:\n",
    "        Tensor of shape (n_samples, 2).\n",
    "    \"\"\"\n",
    "    t = 1.5 * np.pi * (1 + 2 * torch.rand(n_samples))\n",
    "    x = t * torch.cos(t)\n",
    "    y = t * torch.sin(t)\n",
    "    data = torch.stack([x, y], dim=1) / 15.0  # Scale down the data\n",
    "    return data.to(device)\n",
    "\n",
    "# Function to sample from a base Gaussian distribution.\n",
    "def sample_gaussian(n_samples, device='cpu'):\n",
    "    return torch.randn(n_samples, 2, device=device)\n",
    "\n",
    "# Visualize the target Swiss Roll:\n",
    "data = generate_swiss_roll(n_samples=10000)\n",
    "plt.scatter(data[:, 0].cpu(), data[:, 1].cpu(), alpha=0.5, s=10)\n",
    "plt.title(\"Synthetic Swiss Roll Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a3ad2c-1069-4f4d-836e-5f293b40d10c",
   "metadata": {},
   "source": [
    "\n",
    "## Flow Matching Network Architecture\n",
    "\n",
    "This network takes as input a 2D point $x \\in \\mathbb{R}^2$ and a scalar time $t$ (reshaped to $\\mathbb{R}^1$).  \n",
    "It outputs a 2D velocity $v(x,t) \\in \\mathbb{R}^2$.\n",
    "\n",
    "The network architecture is a simple MLP.\n",
    "\n",
    "**Note:** You might experiment with additional layers, activations, or even different network architectures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497e0aa0-7141-4d91-96ed-3e1fef2fa7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlowMatchingNet(nn.Module):\n",
    "    def __init__(self, in_dim=3, hidden_dim=128, out_dim=2):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_dim: Dimensionality of the input (2 for x and 1 for t).\n",
    "            hidden_dim: Hidden layer dimension.\n",
    "            out_dim: Dimensionality of the output (2 for velocity).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, out_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        # x has shape (B, 2) and t has shape (B, 1). Concatenate them along dimension 1.\n",
    "        xt = torch.cat([x, t], dim=1)\n",
    "        return self.net(xt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2a18a8-d866-4325-90d3-f25dd3fdba6b",
   "metadata": {},
   "source": [
    "\n",
    "## Training the Flow Matching Model\n",
    "\n",
    "We now train the network using the flow matching objective.\n",
    "\n",
    "**Procedure:**\n",
    "1. Sample a random time $t \\sim U(0,1)$ for each data point.\n",
    "2. Sample $x_0$ from the base Gaussian and $x_1$ from the Swiss Roll.\n",
    "3. Compute the linear interpolation:\n",
    "   $$\n",
    "   x_t = (1-t)x_0 + t x_1.\n",
    "   $$\n",
    "4. Define the ground-truth velocity as:\n",
    "   $$\n",
    "   v_{\\text{target}} = x_1 - x_0.\n",
    "   $$\n",
    "5. Compute the network's prediction $v_{\\text{pred}} = v(x_t,t)$.\n",
    "6. Compute the mean squared error between $v_{\\text{pred}}$ and $v_{\\text{target}}$.\n",
    "\n",
    "**TODO:** You may experiment with modifying the training loop (e.g., change the number of steps, batch size, or learning rate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f906b57-6b1e-4198-9587-24f40716bbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_flow_matching(net, optimizer, num_steps=10000, batch_size=512, device='cpu'):\n",
    "    \"\"\"\n",
    "    Train the flow matching network.\n",
    "    \n",
    "    Args:\n",
    "        net: The FlowMatchingNet.\n",
    "        optimizer: Optimizer for training.\n",
    "        num_steps: Number of training steps.\n",
    "        batch_size: Batch size.\n",
    "        device: Device to use.\n",
    "    \"\"\"\n",
    "    for step in range(num_steps):\n",
    "        # Sample a batch of times uniformly in [0, 1]\n",
    "        t = torch.rand(batch_size, 1, device=device)\n",
    "        # Sample x0 (base) and x1 (target) from their respective distributions.\n",
    "        x0 = sample_gaussian(batch_size, device=device)\n",
    "        x1 = generate_swiss_roll(batch_size, device=device)\n",
    "        \n",
    "        # Compute the linear interpolation:\n",
    "        #   x_t = (1-t) * x0 + t * x1.\n",
    "        x_t = (1 - t) * x0 + t * x1\n",
    "        \n",
    "        # The ground-truth velocity is constant along the interpolation:\n",
    "        v_target = x1 - x0\n",
    "        \n",
    "        # Network prediction: velocity given x_t and t.\n",
    "        v_pred = net(x_t, t)\n",
    "        \n",
    "        # Compute mean squared error loss.\n",
    "        loss = ((v_pred - v_target) ** 2).mean()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if step % 500 == 0:\n",
    "            print(f\"Step {step}, Loss: {loss.item():.6f}\")\n",
    "    \n",
    "    print(\"Training completed.\")\n",
    "\n",
    "# Initialize network and optimizer.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "net = FlowMatchingNet().to(device)\n",
    "optimizer = optim.Adam(net.parameters(), lr=1e-3)\n",
    "\n",
    "# Train the flow matching network.\n",
    "train_flow_matching(net, optimizer, num_steps=10000, batch_size=256, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da80c7b6-da27-4c3f-bd28-275a6fa99554",
   "metadata": {},
   "source": [
    "\n",
    "## Sampling from the Learned Flow\n",
    "\n",
    "To generate new samples from the target distribution we simulate the ODE:\n",
    "$$\n",
    "\\frac{dx}{dt} = v(x,t),\n",
    "$$\n",
    "using Euler integration. We start from a base sample $x(0) \\sim \\mathcal{N}(0,I)$ at $t=0$ and\n",
    "integrate from $t=0$ to $t=1$.\n",
    "\n",
    "**TODO:** Experiment with a more advanced solver (e.g., Runge-Kutta) to see if the sample quality improves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0b9460-788c-4370-af56-ea6ba4f28f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [code]\n",
    "def sample_from_flow(net, num_samples=1000, num_steps=100, device='cpu'):\n",
    "    \"\"\"\n",
    "    Generate samples using the learned flow.\n",
    "    \n",
    "    Args:\n",
    "        net: The trained FlowMatchingNet.\n",
    "        num_samples: Number of samples to generate.\n",
    "        num_steps: Number of integration steps.\n",
    "        device: Device to use.\n",
    "        \n",
    "    Returns:\n",
    "        Tensor of shape (num_samples, 2).\n",
    "    \"\"\"\n",
    "    dt = 1.0 / num_steps  # Time step for Euler integration.\n",
    "    # Start from the base distribution.\n",
    "    x = sample_gaussian(num_samples, device=device)\n",
    "    \n",
    "    # Integrate forward in time from t=0 to t=1.\n",
    "    for step in range(num_steps):\n",
    "        # Current time (scalar) and expand to a (num_samples,1) tensor.\n",
    "        t = (step * dt) * torch.ones(num_samples, 1, device=device)\n",
    "        # Compute the velocity via the network.\n",
    "        v = net(x, t)\n",
    "        # Euler update: x_{t+dt} = x_t + dt * v(x_t, t)\n",
    "        x = x + dt * v\n",
    "\n",
    "        # TODO: Replace the Euler update with a Runge-Kutta 4 (RK4) integration method.\n",
    "        # For RK4, compute:\n",
    "        #   k1 = dt * v(x, t)\n",
    "        #   k2 = dt * v(x + 0.5*k1, t + 0.5*dt)\n",
    "        #   k3 = dt * v(x + 0.5*k2, t + 0.5*dt)\n",
    "        #   k4 = dt * v(x + k3, t + dt)\n",
    "        # Then update: x = x + (k1 + 2*k2 + 2*k3 + k4) / 6\n",
    "        # This is left as an exercise to better understand numerical ODE solvers.\n",
    "    return x\n",
    "\n",
    "# Generate samples using the learned flow.\n",
    "samples_flow = sample_from_flow(net, num_samples=1000, num_steps=100, device=device)\n",
    "samples_flow = samples_flow.detach().cpu().numpy()\n",
    "\n",
    "# Plot the generated samples.\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.scatter(samples_flow[:, 0], samples_flow[:, 1], s=5, alpha=0.6)\n",
    "plt.title(\"Samples from the Learned Flow\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b634dec0-b770-44b0-a7f9-0742b07e7a96",
   "metadata": {},
   "source": [
    "## Comparing Flow Matching and Diffusion Objectives\n",
    "\n",
    "In the flow matching framework, the network is trained to predict a *constant* velocity field $x_1-x_0$ along the interpolant.\n",
    "In diffusion models, the network is instead trained to predict the noise added at each time step (with time‚Äêdependent schedules).\n",
    "\n",
    "**Your Tasks:**\n",
    "- Compare sample quality between flow matching and diffusion approaches.\n",
    "- Experiment with different training settings in either framework.\n",
    "- Reflect on advantages and limitations of each approach for generative modeling.\n",
    "\n",
    "Happy experimenting!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13764984-568b-4b82-9721-66ca16043fba",
   "metadata": {},
   "source": [
    "# Extra Resource for Reference \n",
    "\n",
    "## Understanding Flow Matching\n",
    "\n",
    "Imagine you're watching a cloud morph into a shape in the sky. The transformation happens smoothly - parts of the cloud gradually shift and move until the final shape emerges. Flow matching works in a similar way: it learns how to smoothly transform one distribution (like random noise) into another (like our desired data).\n",
    "\n",
    "## Why Do We Need Flow Matching?\n",
    "\n",
    "Traditional generative models can be tricky. GANs often have unstable training (imagine two artists constantly arguing about what makes a good painting), and VAEs might be lossy. Flow matching offers a more direct and stable approach.\n",
    "\n",
    "## The Intuition Behind Flow Matching\n",
    "\n",
    "1. Every point in our space has to move from its starting position (noise) to its final position (data)\n",
    "2. We want these movements to be smooth and coordinated\n",
    "3. At each moment, each point needs to know which direction to move\n",
    "\n",
    "### The Mathematical Breakdown\n",
    "\n",
    "Let's break this down step by step:\n",
    "\n",
    "1. **The Velocity Field**: At each point in space and time, we need to know \"which way to move\":\n",
    "   $$\\frac{dx(t)}{dt} = v(x(t), t)$$\n",
    "   This is like having a dance instructor at every spot in the room, telling dancers which way to move.\n",
    "\n",
    "2. **The Path**: We use a simple linear path between start and end:\n",
    "   $$x(t) = \\alpha(t)x_0 + (1-\\alpha(t))x_1$$\n",
    "   \n",
    "   Think of it like a rope between two points - as you move along the rope ($t$ going from 0 to 1), you smoothly go from one end to the other.\n",
    "\n",
    "## How Do We Train This? \n",
    "\n",
    "### The Learning Process\n",
    "\n",
    "1. **Start Simple**: Begin with random noise (imagine scattered dots)\n",
    "2. **Define the Goal**: Know what we want these dots to become (our Swiss Roll pattern)\n",
    "3. **Learn the Flow**: Train a neural network to guide each point along its path\n",
    "\n",
    "### The Training\n",
    "\n",
    "For each training step:\n",
    "1. Take some random points (noise)\n",
    "2. Pick random moments in time\n",
    "3. Figure out where points should be at those times\n",
    "4. Teach our network to predict the right movements\n",
    "\n",
    "Mathematically, we minimize:\n",
    "$$\\mathcal{L} = \\mathbb{E}_{x_0,x_1,t}[\\|v_\\theta(x(t),t) - v^*(x,t)\\|^2]$$\n",
    "\n",
    "\n",
    "## Implementation Overview\n",
    "\n",
    "In this tutorial, we'll:\n",
    "1. Create a Swiss Roll pattern (our target distribution)\n",
    "2. Build a neural network (our flow controller - tells every point which direction to move)\n",
    "3. Train it to learn the right flow\n",
    "4. Watch how points flow from noise to beautiful patterns!\n",
    "\n",
    "Below, let's start by generating our Swiss Roll pattern...\n",
    "\n",
    "## Key Papers for more details üìö\n",
    "\n",
    "* Lipman (2023). \"Flow Matching for Generative Modeling.\" ICLR 2023\n",
    "* Chen et al. (2018). \"Neural Ordinary Differential Equations.\" NeurIPS 2018\n",
    "* Grathwohl et al. (2019). \"FFJORD: Free-form Continuous Dynamics.\" ICLR 2019"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
