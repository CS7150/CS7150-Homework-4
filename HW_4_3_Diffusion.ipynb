{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95ccfb1b-e5de-41ce-8ced-8ee336abec3c",
   "metadata": {},
   "source": [
    "\n",
    "# Diffusion Model Homework Notebook\n",
    "\n",
    "In this notebook, you will work with a diffusion model on a generated Swiss Roll dataset. Our overall goals are to:\n",
    "- Understand how to embed time in a neural network.\n",
    "- Calculate the diffusion (forward) and reverse processes.\n",
    "- Compute the loss based on the discrepancy between predicted noise and the true noise.\n",
    "\n",
    "> **Key Learning Objectives:**\n",
    "> 1. **Loss Computation (Theory & Code):**  \n",
    ">    Understand and implement the loss:  \n",
    "$$\n",
    "      \\mathcal{L} = \\frac{1}{N}\\sum_{i=1}^{N} \\left\\Vert \\hat{\\epsilon}_i - \\epsilon_i \\right\\Vert^2\n",
    "$$\n",
    ">    where $\\hat{\\epsilon}$ is the noise predicted by the model, and $\\epsilon$ is the actual noise added in the forward process.\n",
    "> \n",
    "> 2. **Noise Scheduling and Reverse Process:**  \n",
    ">    Understand the roles of the beta schedule and the computation of the reverse process steps.\n",
    "> \n",
    "> 3. **Practical Implementation:**  \n",
    ">   Write and debug the parts of the code where you fill in the redacted parts based on the mathematical formulas.\n",
    "\n",
    "Let's get started with the necessary imports and setting of random seeds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7089b4d-4200-477f-a918-ecf765b49157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and random seed setup\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn  \n",
    "import numpy as np   \n",
    "import matplotlib.pyplot as plt  \n",
    "from tqdm import tqdm  \n",
    "import math  \n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)  # Ensures that the PyTorch random generators are deterministic\n",
    "np.random.seed(42)     # Ensures that the NumPy random generators are deterministic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64747a9-09cb-433c-9290-15bef1f6f1c1",
   "metadata": {},
   "source": [
    "## Swiss Roll Data Generation\n",
    "The following cell contains a function to generate a 2D Swiss Roll, which will be our training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd798617-a3b6-4cde-aa25-6d2d364762ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_swiss_roll(n_samples=1000):\n",
    "    \"\"\"\n",
    "    Generate 2D Swiss Roll data points.\n",
    "    \n",
    "    Args:\n",
    "        n_samples: Number of points to generate.\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor of shape (n_samples, 2)\n",
    "    \"\"\"\n",
    "    # Generate random angles for the spirals\n",
    "    t = 1.5 * np.pi * (1 + 2 * torch.rand(n_samples))  # Random parameter t\n",
    "    # Compute x and y using cosine and sine\n",
    "    x = t * torch.cos(t)  # x-coordinate of the Swiss roll\n",
    "    y = t * torch.sin(t)  # y-coordinate of the Swiss roll\n",
    "    \n",
    "    # Stack x and y together and scale the data to approximately fit within [-1, 1]\n",
    "    data = torch.stack([x, y], dim=1) / 15.0  \n",
    "    return data\n",
    "\n",
    "# Example call\n",
    "sample_data = generate_swiss_roll()\n",
    "print(sample_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e455547-4920-4705-bbcf-a7321746eeec",
   "metadata": {},
   "source": [
    "## Defining the Time Embedding Module\n",
    "\n",
    "The next module embeds a scalar time-step into a higher dimensional space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a45700-a523-4458-9da0-bda390a0a15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeEmbedding(nn.Module):\n",
    "    \"\"\"Time embedding layer converts a scalar time input into a higher dimensional embedding.\"\"\"\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        # Define a simple MLP with two linear transformations and a GELU activation.\n",
    "        self.embed = nn.Sequential(\n",
    "            nn.Linear(1, dim),  # First linear layer\n",
    "            nn.GELU(),          # GELU activation for non-linearity\n",
    "            nn.Linear(dim, dim) # Second linear layer\n",
    "        )\n",
    "        \n",
    "    def forward(self, t):\n",
    "        # Ensure the time tensor has a feature dimension.\n",
    "        t = t.unsqueeze(-1).float()  # Reshape to (batch_size, 1)\n",
    "        return self.embed(t)  # Return the time embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01cb3cd-df60-452b-afb8-a245cf2af1e5",
   "metadata": {},
   "source": [
    "## Diffusion Model Architecture\n",
    "\n",
    "This cell defines the MLP-based diffusion model. It uses the time embedding and includes residual connections and scaling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32794b8-316a-4d6b-9cb9-111fa10778bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffusionModel(nn.Module):\n",
    "    \"\"\"MLP-based diffusion model with time embedding for Swiss Roll data.\"\"\"\n",
    "    def __init__(self, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Time embedding module to incorporate time information into the model.\n",
    "        self.time_embed = TimeEmbedding(hidden_dim)\n",
    "        \n",
    "        # Initial input layer transforms 2D data to the hidden dimension.\n",
    "        self.input_layer = nn.Sequential(\n",
    "            nn.Linear(2, hidden_dim),\n",
    "            nn.GELU()\n",
    "        )\n",
    "        \n",
    "        # Hidden layers with added time embedding at each step and residual connections.\n",
    "        self.hidden_layers = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(hidden_dim, hidden_dim),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(hidden_dim, hidden_dim),\n",
    "                nn.GELU()\n",
    "            ) for _ in range(3)\n",
    "        ])\n",
    "        \n",
    "        # Final output layer brings the features back to 2D.\n",
    "        self.output_layer = nn.Linear(hidden_dim, 2)\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        # Compute the time embedding from the given time input.\n",
    "        t_emb = self.time_embed(t)\n",
    "        \n",
    "        # Process input data through the first layer.\n",
    "        h = self.input_layer(x)\n",
    "        # Add the time embedding to the layer activation.\n",
    "        h = h + t_emb\n",
    "        \n",
    "        # Loop over each hidden layer:\n",
    "        for layer in self.hidden_layers:\n",
    "            h_prev = h  # Save current h for residual connection.\n",
    "            h = layer(h)  # Process with current hidden layer.\n",
    "            h = h + h_prev  # Residual connection.\n",
    "            h = h + t_emb   # Add time embedding at each hidden layer.\n",
    "            h = h / math.sqrt(2)  # Normalize the activation.\n",
    "            \n",
    "        # Final output transformation to reconstruct the 2D output.\n",
    "        x = self.output_layer(h)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aad72da-b36f-4a8c-8f73-e3da31ce809d",
   "metadata": {},
   "source": [
    "## Diffusion Scheduler and Reverse Process\n",
    "\n",
    "This cell defines the noise scheduler responsible for scheduling beta parameters over timesteps. It \n",
    "also includes methods for the forward (diffuse) and reverse (denosing) processes.\n",
    "\n",
    "**Note:** The reverse process relies on a core computation matching the theoretical formula for denoising.  \n",
    "For the reverse step, the formula is given as:\n",
    "\n",
    "$$\n",
    "x_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}} \\left( x_t - \\frac{1-\\alpha_t}{\\sqrt{1-\\bar{\\alpha}_t}} \\hat{\\epsilon} \\right)\n",
    "+ \\sqrt{\\beta_t} \\cdot \\mathbf{z}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\alpha_t = 1 - \\beta_t$\n",
    "- $\\bar{\\alpha}_t = \\prod_{s=0}^{t} \\alpha_s$\n",
    "- $\\hat{\\epsilon}$ is the predicted noise.\n",
    "- $\\mathbf{z}$ is standard Gaussian noise (only if $t > 0$).\n",
    "\n",
    "The implementation in the code cell has a `TODO` where you need to implement this formula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd644d8-517f-44c4-92e8-a7fa117f3c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffusionScheduler:\n",
    "    \"\"\"Scheduler that handles noise scheduling for the diffusion model.\"\"\"\n",
    "    def __init__(self, num_timesteps=1000, device='cpu'):\n",
    "        self.num_timesteps = num_timesteps\n",
    "        self.device = device\n",
    "        \n",
    "        # Create an improved beta scheduling linearly spaced\n",
    "        self.betas = torch.linspace(1e-4, 0.02, num_timesteps).to(device)  # Beta schedule\n",
    "        self.alphas = 1. - self.betas  # Define alphas from betas\n",
    "        self.alphas_cumprod = torch.cumprod(self.alphas, dim=0)  # Cumulative product of alphas\n",
    "        \n",
    "        # Precompute square roots for speed\n",
    "        self.sqrt_alphas_cumprod = torch.sqrt(self.alphas_cumprod)\n",
    "        self.sqrt_one_minus_alphas_cumprod = torch.sqrt(1. - self.alphas_cumprod)\n",
    "        \n",
    "        # Reverse process parameters\n",
    "        self.sqrt_recip_alphas = torch.sqrt(1.0 / self.alphas)\n",
    "        \n",
    "        # For posterior variance, compute alphas_cumprod for previous timestep (with a 1 at the start)\n",
    "        alphas_cumprod_prev = torch.cat([torch.ones(1).to(device), self.alphas_cumprod[:-1]])\n",
    "        self.posterior_variance = self.betas * (1. - alphas_cumprod_prev) / (1. - self.alphas_cumprod)\n",
    "    \n",
    "    def diffuse(self, x_0, t):\n",
    "        \"\"\"\n",
    "        Forward diffusion: add noise to the data.\n",
    "        \n",
    "        Args:\n",
    "            x_0: Original data.\n",
    "            t: Time step indices.\n",
    "            \n",
    "        Returns:\n",
    "            Noisy data x_t and the noise that was added.\n",
    "        \"\"\"\n",
    "        noise = torch.randn_like(x_0)  # Generate Gaussian noise\n",
    "        \n",
    "        # Get scaling terms for the chosen timesteps\n",
    "        sqrt_alpha_t = self.sqrt_alphas_cumprod[t].view(-1, 1)  # Scale for clean data\n",
    "        sqrt_one_minus_alpha_t = self.sqrt_one_minus_alphas_cumprod[t].view(-1, 1)  # Scale for noise\n",
    "        \n",
    "        # x_t according to the diffusion process equation\n",
    "        x_t = sqrt_alpha_t * x_0 + sqrt_one_minus_alpha_t * noise\n",
    "        return x_t, noise\n",
    "    \n",
    "    def reverse_step(self, x_t, t, predicted_noise):\n",
    "        \"\"\"\n",
    "        Reverse process (denoising) for a single timestep.\n",
    "        \n",
    "        **Core Computation:**\n",
    "        \\[\n",
    "        x_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}} \\left( x_t - \\frac{1-\\alpha_t}{\\sqrt{1-\\bar{\\alpha}_t}} \\hat{\\epsilon} \\right)\n",
    "        + \\sqrt{\\beta_t} \\cdot \\mathbf{z}\n",
    "        \\]\n",
    "        where \\(\\mathbf{z} \\sim \\mathcal{N}(0, I)\\) if \\(t > 0\\), else \\(\\mathbf{z} = 0\\).\n",
    "        \n",
    "        **TODO:** Implement the equation above.\n",
    "        \n",
    "        **Hint:**\n",
    "        - Use `self.alphas[t]` for \\(\\alpha_t\\).\n",
    "        - Use `self.alphas_cumprod[t]` for \\(\\bar{\\alpha}_t\\).\n",
    "        - For noise, use `torch.randn_like(x_t)` if `t > 0` else zero.\n",
    "        \"\"\"\n",
    "        alpha = self.alphas[t]  # Get \\alpha_t\n",
    "        alpha_bar = self.alphas_cumprod[t]  # Get \\bar{\\alpha}_t\n",
    "        \n",
    "        # TODO: Implement the reverse step update based on the formula given in the markdown above.\n",
    "        # For now, we leave this as a stub for the you to complete.\n",
    "        # Expected steps:\n",
    "        #   1. Compute the term: (x_t - coeff * predicted_noise)\n",
    "        #   2. Scale the term by 1/sqrt(alpha)\n",
    "        #   3. If t > 0, add noise scaled by sqrt(beta)\n",
    "        \n",
    "        if t > 0:\n",
    "            noise = torch.randn_like(x_t)  # Noise term only if not the last timestep.\n",
    "        else:\n",
    "            noise = 0\n",
    "        \n",
    "        # ----- TODO Implementation Starts Here -----\n",
    "        #\n",
    "        # Compute the first term: 1/sqrt(alpha) * (x_t - (1 - alpha)/sqrt(1 - alpha_bar) * predicted_noise)\n",
    "        # Then, add sqrt(beta_t) * noise\n",
    "        #\n",
    "        # Example (pseudocode):\n",
    "        # coeff = (1 - alpha) / sqrt(1 - alpha_bar)\n",
    "        # x_t_minus_1 = (1 / sqrt(alpha)) * (x_t - coeff * predicted_noise) + sqrt(betas[t]) * noise\n",
    "        #\n",
    "        # ----- TODO Implementation Ends Here -----\n",
    "        \n",
    "        # ---- Placeholder value below ----\n",
    "        # Remove the following line after implementing the code.\n",
    "        raise NotImplementedError(\"Reverse step function is not implemented. Please complete the TODO part based on the provided formula.\")\n",
    "        \n",
    "        # return x_t_minus_1  # Uncomment when implementation is complete."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341f2d2f-681b-4308-9820-8811f26dd2f8",
   "metadata": {},
   "source": [
    "## Training the Diffusion Model\n",
    "\n",
    "In this cell, we define the training loop for our diffusion model.\n",
    "One critical step is computing the MSE loss between the predicted noise and the true noise.\n",
    "Recall that the loss is defined as:\n",
    "$$\n",
    "\\mathcal{L} = \\frac{1}{N}\\sum_{i=1}^{N} \\left\\Vert \\hat{\\epsilon}_i - \\epsilon_i \\right\\Vert^2\n",
    "$$\n",
    "The corresponding code line should match this formula.\n",
    "\n",
    "For the training loop, some parts remain intact.  \n",
    "Feel free to adjust the number of training steps or the learning rate as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92af8d92-d84c-4f1c-b9ab-64d3004f1e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_diffusion(n_steps=10000, batch_size=128, lr=1e-3):\n",
    "    \"\"\"Train the diffusion model on Swiss Roll data\"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Initialize the diffusion model and move it to the chosen device.\n",
    "    model = DiffusionModel().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    # Initialize the scheduler\n",
    "    scheduler = DiffusionScheduler(device=device)\n",
    "    \n",
    "    losses = []  # To store the loss at each step\n",
    "    pbar = tqdm(range(n_steps), desc=\"Training Diffusion\")\n",
    "    \n",
    "    for step in pbar:\n",
    "        # Generate a batch of Swiss Roll data.\n",
    "        x_0 = generate_swiss_roll(batch_size).to(device)\n",
    "        \n",
    "        # Sample random time steps for each data point.\n",
    "        t = torch.randint(0, scheduler.num_timesteps, (x_0.shape[0],)).to(device)\n",
    "        \n",
    "        # Get the noisy data and the noise using the forward process.\n",
    "        x_t, noise = scheduler.diffuse(x_0, t)\n",
    "        \n",
    "        # Predict the noise from the model.\n",
    "        predicted_noise = model(x_t, t.float() / scheduler.num_timesteps)\n",
    "        \n",
    "        # Compute the loss as the MSE between the predicted noise and the actual noise.\n",
    "        loss = torch.mean((predicted_noise - noise) ** 2)  # Matches the formula \\(\\frac{1}{N} \\sum_i \\|\\hat{\\epsilon}_i - \\epsilon_i\\|^2\\)\n",
    "        \n",
    "        optimizer.zero_grad()  # Zero the gradients.\n",
    "        loss.backward()  # Backpropagate the loss.\n",
    "        optimizer.step()  # Update the parameters.\n",
    "        \n",
    "        losses.append(loss.item())  # Log the loss for visualization.\n",
    "        pbar.set_postfix({'loss': loss.item()})\n",
    "    \n",
    "    return model, losses, scheduler, device\n",
    "\n",
    "# You can adjust parameters or test this training function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3206ad7b-09a2-4536-b58d-f08dd004c703",
   "metadata": {},
   "source": [
    "## Generating and Visualizing Samples\n",
    "\n",
    "After training, we will generate samples from our diffusion model starting from pure noise, and then visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f231095b-ae29-43c2-9d47-3edb6ef3d5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_samples(model, scheduler, n_samples=1000):\n",
    "    \"\"\"Generate Swiss Roll samples from noise using the reverse process of the diffusion model.\"\"\"\n",
    "    device = next(model.parameters()).device  # Get device from model parameters\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    \n",
    "    x = torch.randn(n_samples, 2).to(device)  # Start from random noise\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Reverse the diffusion process from t = num_timesteps-1 down to t = 0\n",
    "        for step in tqdm(range(scheduler.num_timesteps-1, -1, -1), desc=\"Sampling\"):\n",
    "            t = torch.full((n_samples,), step, device=device, dtype=torch.long)\n",
    "            predicted_noise = model(x, t.float() / scheduler.num_timesteps)\n",
    "            # Use the reverse_step function implemented above to get x at the previous timestep.\n",
    "            x = scheduler.reverse_step(x, step, predicted_noise)\n",
    "    \n",
    "    return x.cpu()\n",
    "\n",
    "def visualize_samples(samples):\n",
    "    \"\"\"Visualize generated samples as a scatter plot.\"\"\"\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.scatter(samples[:, 0], samples[:, 1], alpha=0.5, s=1)\n",
    "    plt.axis('equal')\n",
    "    plt.title('Generated Swiss Roll Samples')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7375728-e95c-4f45-852b-7e84bd020bd4",
   "metadata": {},
   "source": [
    "## Main Execution\n",
    "\n",
    "This final cell ties everything together:  \n",
    "1. Training the diffusion model.  \n",
    "2. Plotting the training loss.  \n",
    "3. Generating and visualizing new samples.\n",
    "\n",
    "**Note:** Make sure the reverse process is correctly implemented before running the sampling step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea974f7-1652-4df0-a61e-279e84dc0026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the diffusion model\n",
    "model, losses, scheduler, device = train_diffusion(n_steps=10000)\n",
    "\n",
    "# Plot the training loss curve\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.show()\n",
    "\n",
    "# Generate samples using the trained model\n",
    "try:\n",
    "    samples = generate_samples(model, scheduler, n_samples=10000)\n",
    "    visualize_samples(samples)\n",
    "except NotImplementedError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d23744a-8748-464c-8768-e2f80be558a3",
   "metadata": {},
   "source": [
    "## Visualizing the Forward Diffusion Process\n",
    "\n",
    "In the forward process, we gradually add noise to the data. In the visualization below, the data is shown at several key timesteps.  \n",
    "\n",
    "**Steps explained:**\n",
    "1. **Generate Initial Data:** We start with the clean Swiss Roll dataset.\n",
    "2. **Select Timesteps:** We choose a few key timesteps (e.g., 0%, 2%, 5%, 10%, 20%, 50%, and 100% of the max timestep) to demonstrate the gradual corruption.\n",
    "3. **Apply Diffusion:** For each selected timestep, we compute the noisy data using `scheduler.diffuse()`.\n",
    "4. **Visualization:** A scatter plot of the noisy data is displayed for each timestep.\n",
    "\n",
    "> **Hint:**  \n",
    "> The diffusion function uses:  \n",
    " $$\n",
    " x_t = \\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1 - \\bar{\\alpha}_t}\\,\\epsilon\n",
    " $$\n",
    "> where $\\epsilon$ is standard Gaussian noise.  \n",
    "> Check comments in the code for further guidance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd17588e-8437-45e0-b54f-a7ba5966da72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_forward_process(model, scheduler, n_samples=1000):\n",
    "    \"\"\"\n",
    "    Visualize how data gets progressively noisier in the forward process.\n",
    "    \n",
    "    Args:\n",
    "        model: The diffusion model (not directly used in forward process but kept for consistency).\n",
    "        scheduler: The diffusion scheduler which supplies the noise schedule.\n",
    "        n_samples: Number of data points to use for visualization.\n",
    "    \"\"\"\n",
    "    # Get the device from model parameters.\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    # Generate the initial clean Swiss Roll data.\n",
    "    x_0 = generate_swiss_roll(n_samples).to(device)\n",
    "    \n",
    "    # Prepare a list of timesteps to visualize\n",
    "    # Ensure timesteps do not exceed the scheduler's maximum value.\n",
    "    max_timestep = scheduler.num_timesteps - 1\n",
    "    plot_steps = [\n",
    "        int(t) for t in [0, max_timestep * 0.02, max_timestep * 0.05,\n",
    "                         max_timestep * 0.1,  max_timestep * 0.2,\n",
    "                         max_timestep * 0.5, max_timestep]\n",
    "    ]\n",
    "    \n",
    "    # Create a grid of subplots for each selected timestep.\n",
    "    fig, axes = plt.subplots(1, len(plot_steps), figsize=(15, 3))\n",
    "    \n",
    "    # Loop over each selected timestep to compute and plot the corresponding noisy data.\n",
    "    for idx, t in enumerate(plot_steps):\n",
    "        # Create a tensor of timesteps of size equal to n_samples.\n",
    "        timesteps = torch.full((n_samples,), t, device=device, dtype=torch.long)\n",
    "        # Apply the forward diffusion process: add noise to the data.\n",
    "        x_t, _ = scheduler.diffuse(x_0, timesteps)\n",
    "        \n",
    "        # Plot the noisy data.\n",
    "        ax = axes[idx]\n",
    "        samples = x_t.detach().cpu().numpy()  # Detach from graph and move data to CPU.\n",
    "        ax.scatter(\n",
    "            samples[:, 0], samples[:, 1],\n",
    "            s=10, alpha=0.5, color=plt.cm.viridis(0.1 + 0.8 * idx / len(plot_steps))\n",
    "        )\n",
    "        ax.set_title(f't = {t}')  # Mark the current timestep on the plot.\n",
    "        ax.set(xlim=(-2.3, 2.7), ylim=(-2.4, 2.6))\n",
    "        ax.set_aspect('equal')\n",
    "        ax.set(xticks=[], yticks=[])\n",
    "    \n",
    "    plt.suptitle('Forward Process: Data → Noise', y=1.05)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# You can call this function to visualize the forward process.\n",
    "# visualize_forward_process(model, scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caca420a-c48a-4fac-aba9-cbfd6dcaf1e9",
   "metadata": {},
   "source": [
    "## Visualizing the Reverse Diffusion Process\n",
    "\n",
    "In the reverse process, the model starts from pure noise and progressively denoises the data until a coherent structure (the Swiss Roll) is recovered.\n",
    "\n",
    "**Steps explained:**\n",
    "1. **Start from Noise:** Begin with pure noise as \\( x_T \\).\n",
    "2. **Reverse Diffusion:** At each reverse step \\( t \\rightarrow t-1 \\), apply the denoising step using `scheduler.reverse_step()`.\n",
    "3. **Save Snapshots:** At selected timesteps, save the generated data to visualize the denoising trajectory.\n",
    "4. **Visualization:** The plots show how the model’s output changes from random noise to structured data.\n",
    "\n",
    "> **Hint:**  \n",
    "> The reverse step is based on:  \n",
    "$$\n",
    " x_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}} \\left( x_t - \\frac{1-\\alpha_t}{\\sqrt{1-\\bar{\\alpha}_t}} \\,\\hat{\\epsilon} \\right) + \\sqrt{\\beta_t}\\,\\mathbf{z}\n",
    "$$\n",
    "> where $\\mathbf{z}$ is Gaussian noise (only when $ t > 0 $)\n",
    "> \n",
    "> Ensure your `reverse_step` function implementation is correct before visualizing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e700f39-eaa2-4054-bf8f-bc9be118b22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_reverse_process(model, scheduler, n_samples=1000):\n",
    "    \"\"\"\n",
    "    Visualize the generation (reverse diffusion) process, showing the progressive denoising.\n",
    "    \n",
    "    Args:\n",
    "        model: The diffusion model trained for denoising.\n",
    "        scheduler: The scheduler that contains the reverse process implementation.\n",
    "        n_samples: Number of samples to generate and visualize.\n",
    "    \"\"\"\n",
    "    # Get the device from the model.\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    # Start from pure noise.\n",
    "    x = torch.randn(n_samples, 2).to(device)\n",
    "    \n",
    "    # Define selected timesteps to capture snapshots from the denoising process.\n",
    "    max_timestep = scheduler.num_timesteps - 1\n",
    "    plot_steps = [\n",
    "        int(t) for t in [max_timestep, max_timestep * 0.5, max_timestep * 0.2,\n",
    "                         max_timestep * 0.1, max_timestep * 0.05, max_timestep * 0.02, 0]\n",
    "    ]\n",
    "    # Dictionary to save samples at chosen timesteps.\n",
    "    samples_to_save = {step: None for step in plot_steps}\n",
    "    \n",
    "    # Create subplots for visualization.\n",
    "    fig, axes = plt.subplots(1, len(plot_steps), figsize=(15, 3))\n",
    "    \n",
    "    # Reverse diffusion process loop.\n",
    "    with torch.no_grad():\n",
    "        # Iterate from the last timestep backwards.\n",
    "        for step in tqdm(range(scheduler.num_timesteps - 1, -1, -1), desc=\"Sampling\"):\n",
    "            # Create a tensor of the current timestep for all samples.\n",
    "            t = torch.full((n_samples,), step, device=device, dtype=torch.long)\n",
    "            # Predict the noise for the current step.\n",
    "            predicted_noise = model(x, t.float() / scheduler.num_timesteps)\n",
    "            # Apply the reverse step to obtain data from the previous timestep.\n",
    "            x = scheduler.reverse_step(x, step, predicted_noise)\n",
    "            \n",
    "            # If the current step is a designated snapshot, save the current data.\n",
    "            if step in plot_steps:\n",
    "                idx = plot_steps.index(step)  # Identify subplot index.\n",
    "                ax = axes[idx]\n",
    "                samples = x.detach().cpu().numpy()  # Convert tensor for plotting.\n",
    "                ax.scatter(\n",
    "                    samples[:, 0], samples[:, 1],\n",
    "                    s=10, alpha=0.5, color=plt.cm.magma(0.1 + 0.8 * idx / len(plot_steps))\n",
    "                )\n",
    "                ax.set_title(f't = {step}')\n",
    "                ax.set(xlim=(-2.3, 2.7), ylim=(-2.4, 2.6))\n",
    "                ax.set_aspect('equal')\n",
    "                ax.set(xticks=[], yticks=[])\n",
    "    \n",
    "    plt.suptitle('Reverse Process: Noise → Data', y=1.05)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# You can call this function to visualize the reverse process.\n",
    "# visualize_reverse_process(model, scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5771952d-52a5-4475-92d7-6687e1f19750",
   "metadata": {},
   "source": [
    "\n",
    "## Comparing Real and Generated Samples\n",
    "\n",
    "In this final visualization, we compare the original Swiss Roll data with the data generated by the diffusion model.  \n",
    "This side-by-side view offers insight into how effectively the diffusion model recovers the original data distribution.  \n",
    "\n",
    "**Steps explained:**\n",
    "- **Real Data:** We generate a fresh batch of Swiss Roll data.\n",
    "- **Generated Data:** We use the trained diffusion model and the reverse process to produce data starting from noise.\n",
    "- **Comparison:** Display both plots side-by-side for an intuitive visual comparison.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aba0b97-584d-48dd-aa9a-7e91ee97bc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare real and generated samples side-by-side.\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot the original (real) Swiss Roll data.\n",
    "plt.subplot(1, 2, 1)\n",
    "real_data = generate_swiss_roll(1000)\n",
    "plt.scatter(real_data[:, 0], real_data[:, 1],\n",
    "            c='blue', alpha=0.5, label='Real')\n",
    "plt.title('Real Swiss Roll')\n",
    "plt.legend()\n",
    "plt.axis('equal')\n",
    "plt.xlim(-2.3, 2.7)\n",
    "plt.ylim(-2.4, 2.6)\n",
    "\n",
    "# Plot the diffusion-generated data.\n",
    "plt.subplot(1, 2, 2)\n",
    "# Ensure that your reverse_step function is implemented;\n",
    "# otherwise, this might throw a NotImplementedError.\n",
    "try:\n",
    "    samples = generate_samples(model, scheduler)\n",
    "    plt.scatter(samples[:, 0], samples[:, 1],\n",
    "                c='red', alpha=0.5, label='Generated')\n",
    "    plt.title('Diffusion Generated')\n",
    "    plt.legend()\n",
    "    plt.axis('equal')\n",
    "    plt.xlim(-2.3, 2.7)\n",
    "    plt.ylim(-2.4, 2.6)\n",
    "except NotImplementedError as e:\n",
    "    plt.text(0.5, 0.5, str(e), horizontalalignment='center', verticalalignment='center', transform=plt.gca().transAxes)\n",
    "    plt.title('Error in Generation Process')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afdcdf5-b5d4-4310-893d-e172db6e76f0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Final Notes\n",
    "\n",
    "### Implement the reverse_step Function:\n",
    " - The reverse step function contains a placeholder NotImplementedError. Use the provided hints and formula to fill in the missing implementation.\n",
    "   \n",
    "### Documentation and Comments:\n",
    " - Use the inline comments and markdown explanations above to guide your implementation and understanding.\n",
    "   \n",
    "### Testing:\n",
    " - Test your implementation of each module independently before running the full training and sampling process.\n",
    "\n",
    "\n",
    "Good luck, and make sure to reach out if you have any questions about the implementation details or the underlying theory!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
