{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d0698cf-ebde-48df-abc5-8d5855b00ab9",
   "metadata": {},
   "source": [
    "# Understanding RealNVP: Real-valued Non-Volume Preserving Flows\n",
    "\n",
    "Imagine transforming a simple distribution (like a Gaussian) into a complex one (like our Swiss Roll) through a series of invertible transformations. This is exactly what RealNVP does! Unlike diffusion models that gradually add noise, or flow matching that learns velocity fields, RealNVP learns direct invertible mappings between simple and complex distributions.\n",
    "\n",
    "**Key Differences from Other Methods:**\n",
    "- **Diffusion Models**: Add/remove noise gradually over many steps (typically 1000+)\n",
    "- **Flow Matching**: Learn continuous velocity fields for transforming distributions\n",
    "- **RealNVP**: Uses coupling layers for exact, invertible transformations in a single pass\n",
    "\n",
    "## Core Idea: Coupling Layers\n",
    "\n",
    "The fundamental building block of RealNVP is the coupling layer. Each layer:\n",
    "1. Splits input dimensions into two parts using a binary mask\n",
    "2. Keeps one part unchanged\n",
    "3. Transforms the other part using the unchanged part as context\n",
    "\n",
    "The transformation is designed to be both invertible and have an easily computable Jacobian determinant:\n",
    "\n",
    "$$y_{1:d} = x_{1:d} \\odot \\text{mask}$$\n",
    "$$y_{d+1:D} = (1-\\text{mask}) \\odot (x_{d+1:D} \\cdot \\exp(s(x_{1:d})) + t(x_{1:d}))$$\n",
    "\n",
    "where $s(.)$ and $t(.)$ are scale and translation networks.\n",
    "\n",
    "## Why This Works: Change of Variables Formula\n",
    "\n",
    "The key to training normalizing flows like RealNVP is the change of variables formula:\n",
    "\n",
    "$$\\log p_X(x) = \\log p_Z(f(x)) + \\log \\left|\\det\\left(\\frac{\\partial f(x)}{\\partial x}\\right)\\right|$$\n",
    "\n",
    "where:\n",
    "- $p_X(x)$ is the density we want to learn\n",
    "- $p_Z(z)$ is a simple base density (like N(0,I))\n",
    "- $f$ is our invertible transformation\n",
    "- The last term is the log determinant of the Jacobian\n",
    "\n",
    "The coupling layer design ensures this determinant is easily computable:\n",
    "\n",
    "$$\\log \\left|\\det\\left(\\frac{\\partial f(x)}{\\partial x}\\right)\\right| = \\sum_{i=d+1}^D s_i(x_{1:d})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f61bbc-d81a-4e7a-8bfa-2bab095be220",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57016d3b-926c-4153-af38-733d282912a8",
   "metadata": {},
   "source": [
    "## Swiss Roll Data Generation\n",
    "\n",
    "We'll use the same Swiss Roll data generator from our previous examples for consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33668f76-88fd-44a8-aa93-35dbb53b69ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_swiss_roll(n_samples=1000):\n",
    "    \"\"\"\n",
    "    Generate 2D Swiss Roll data points.\n",
    "    \n",
    "    Args:\n",
    "        n_samples: Number of points to generate.\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor of shape (n_samples, 2)\n",
    "    \"\"\"\n",
    "    t = 1.5 * np.pi * (1 + 2 * torch.rand(n_samples))\n",
    "    x = t * torch.cos(t)\n",
    "    y = t * torch.sin(t)\n",
    "    data = torch.stack([x, y], dim=1) / 15.0  # Scale down the data\n",
    "    return data\n",
    "\n",
    "# Example visualization\n",
    "data = generate_swiss_roll(n_samples=10000)\n",
    "plt.scatter(data[:, 0], data[:, 1], alpha=0.5, s=10)\n",
    "plt.title(\"Synthetic Swiss Roll Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c36a67c-070c-453a-96d9-9103eeb6bd4d",
   "metadata": {},
   "source": [
    "## Scale-Translation Network\n",
    "\n",
    "The core of RealNVP's transformation is its Scale-Translation network. This network takes half of the dimensions as input and outputs transformation parameters for the other half.\n",
    "\n",
    "**Network Architecture:**\n",
    "- Input: $x_{1:d}$ (masked dimensions)\n",
    "- Output: $(s, t)$ where:\n",
    "  - $s$: scaling factor (log-scale)\n",
    "  - $t$: translation vector\n",
    "  \n",
    "**Design Considerations:**\n",
    "1. **Expressivity**: Multiple layers allow learning complex relationships\n",
    "2. **Stability**: tanh activation on scale prevents extreme transformations\n",
    "3. **Output Dimension**: Must match the dimensionality of transformed features\n",
    "\n",
    "The network implements the mapping:\n",
    "$$h_\\theta: \\mathbb{R}^d \\rightarrow \\mathbb{R}^{2(D-d)}$$\n",
    "where $\\theta$ represents the network parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62051ff-c462-462e-865d-8527a63be071",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaleTranslateNet(nn.Module):\n",
    "    \"\"\"Network to predict scale and translation parameters.\"\"\"\n",
    "    def __init__(self, in_features, hidden_features, out_features):\n",
    "        super().__init__()\n",
    "        # Define a simple MLP with two hidden layers\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_features, hidden_features),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_features, hidden_features),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_features, out_features)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472659f3-0658-4925-a80d-9c5db469380c",
   "metadata": {},
   "source": [
    "## Coupling Layer: The Heart of RealNVP\n",
    "\n",
    "The coupling layer implements an affine transformation that is:\n",
    "1. **Invertible**: We can exactly recover inputs from outputs\n",
    "2. **Has Tractable Jacobian**: The determinant is easy to compute\n",
    "3. **Expressive**: Can learn complex transformations when stacked\n",
    "\n",
    "**Mathematical Formulation:**\n",
    "\n",
    "Let $\\mathbf{x} \\in \\mathbb{R}^D$ be our input. A coupling layer:\n",
    "\n",
    "1. **Splits** the input using a binary mask $\\mathbf{m}$:\n",
    "   $$\\mathbf{x}_1 = \\mathbf{x} \\odot \\mathbf{m}$$\n",
    "   $$\\mathbf{x}_2 = \\mathbf{x} \\odot (1-\\mathbf{m})$$\n",
    "\n",
    "2. **Transforms** $\\mathbf{x}_2$ conditioned on $\\mathbf{x}_1$:\n",
    "   $$\\mathbf{y}_1 = \\mathbf{x}_1$$\n",
    "   $$\\mathbf{y}_2 = \\mathbf{x}_2 \\odot \\exp(s(\\mathbf{x}_1)) + t(\\mathbf{x}_1)$$\n",
    "\n",
    "3. **Jacobian determinant** is simply:\n",
    "   $$\\log |\\det J| = \\sum_i s_i(\\mathbf{x}_1)$$\n",
    "\n",
    "**TODO:** Implement the forward transformation in the coupling layer.  \n",
    "The transformation should be: y = x_masked + (1 - mask) * (x * exp(s) + t)\n",
    "where s and t are the scale and translation predicted by the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbcf6ad-607b-4684-84c3-e0ed65dae7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CouplingLayer(nn.Module):\n",
    "    \"\"\"A single coupling layer in RealNVP.\"\"\"\n",
    "    def __init__(self, dim, hidden_dim, mask):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.mask = mask  # Binary mask indicating which dimensions remain unchanged\n",
    "        # Network outputs both scale and translation\n",
    "        self.st_net = ScaleTranslateNet(dim, hidden_dim, dim*2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass implementing the affine coupling transform.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, dim)\n",
    "            \n",
    "        Returns:\n",
    "            y: Transformed tensor\n",
    "            log_det_J: Log determinant of the Jacobian\n",
    "        \"\"\"\n",
    "        # Get masked input (unchanged part)\n",
    "        mask = self.mask.to(x.device)\n",
    "        x_masked = x * mask\n",
    "        \n",
    "        # Get scale and translation from network\n",
    "        st = self.st_net(x_masked)\n",
    "        s, t = st.chunk(2, dim=1)\n",
    "        s = torch.tanh(s)  # Stabilize the scale\n",
    "        \n",
    "        # TODO: Implement the forward transformation\n",
    "        # 1. Keep the masked dimensions unchanged\n",
    "        # 2. Transform unmasked dimensions using scale and translation\n",
    "        # 3. Compute log determinant of Jacobian\n",
    "        # Hint: y = x_masked + (1 - mask) * (x * exp(s) + t)\n",
    "        #       log_det_J = sum((1 - mask) * s, dim=1)\n",
    "        \n",
    "        raise NotImplementedError(\"Implement the forward transformation in CouplingLayer\")\n",
    "        \n",
    "        return y, log_det_J\n",
    "\n",
    "    def inverse(self, y):\n",
    "        \"\"\"Inverse transform.\"\"\"\n",
    "        mask = self.mask.to(y.device)\n",
    "        y_masked = y * mask\n",
    "        st = self.st_net(y_masked)\n",
    "        s, t = st.chunk(2, dim=1)\n",
    "        s = torch.tanh(s)\n",
    "        \n",
    "        # Invert the affine transformation\n",
    "        x = y_masked + (1 - mask) * ((y - t) * torch.exp(-s))\n",
    "        log_det_J = -((1 - mask) * s).sum(dim=1)\n",
    "        return x, log_det_J"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07812ab-486e-40c0-8ec2-78601f5a1865",
   "metadata": {},
   "source": [
    "## Complete RealNVP Model\n",
    "\n",
    "The full RealNVP model stacks multiple coupling layers with alternating masks. This ensures that:\n",
    "1. All dimensions can be transformed\n",
    "2. Information can flow between all dimensions\n",
    "3. The transformation remains invertible\n",
    "\n",
    "**Architecture:**\n",
    "$$f = f_K \\circ f_{K-1} \\circ ... \\circ f_1$$\n",
    "\n",
    "where each $f_k$ is a coupling layer with its own mask.\n",
    "\n",
    "**Log-likelihood Computation:**\n",
    "For a base distribution $p_Z(\\mathbf{z})$ (typically $\\mathcal{N}(0,I)$), the log-likelihood is:\n",
    "$$\\log p_X(\\mathbf{x}) = \\log p_Z(f(\\mathbf{x})) + \\sum_{k=1}^K \\log |\\det J_k|$$\n",
    "\n",
    "This can be efficiently computed since each coupling layer's Jacobian determinant is tractable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef0651b-ef78-46f7-b1b5-3e19f56d897d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RealNVP(nn.Module):\n",
    "    \"\"\"Complete RealNVP model with multiple coupling layers.\"\"\"\n",
    "    def __init__(self, dim, hidden_dim, num_coupling_layers):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        \n",
    "        # Create alternating binary masks\n",
    "        masks = []\n",
    "        for i in range(num_coupling_layers):\n",
    "            if i % 2 == 0:\n",
    "                mask = torch.tensor([1 if j < dim//2 else 0 for j in range(dim)])\n",
    "            else:\n",
    "                mask = torch.tensor([0 if j < dim//2 else 1 for j in range(dim)])\n",
    "            masks.append(mask)\n",
    "            \n",
    "        # Stack coupling layers\n",
    "        self.coupling_layers = nn.ModuleList([\n",
    "            CouplingLayer(dim, hidden_dim, mask) for mask in masks\n",
    "        ])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Transform data x to latent z.\"\"\"\n",
    "        log_det_J = 0\n",
    "        for layer in self.coupling_layers:\n",
    "            x, ld = layer(x)\n",
    "            log_det_J += ld\n",
    "        return x, log_det_J\n",
    "    \n",
    "    def inverse(self, z):\n",
    "        \"\"\"Transform latent z back to data x.\"\"\"\n",
    "        log_det_J = 0\n",
    "        for layer in reversed(self.coupling_layers):\n",
    "            z, ld = layer.inverse(z)\n",
    "            log_det_J += ld\n",
    "        return z, log_det_J"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7ee520-f158-461a-8275-bd3497cb1948",
   "metadata": {},
   "source": [
    "## Training the RealNVP Model\n",
    "\n",
    "The training process optimizes the negative log-likelihood (NLL) of the data. For a batch of samples, we:\n",
    "\n",
    "1. **Compute Base Density**: For standard normal base distribution:\n",
    "   $$\\log p_Z(\\mathbf{z}) = -\\frac{1}{2}\\sum_i (z_i^2 + \\log(2\\pi))$$\n",
    "\n",
    "2. **Apply Change of Variables**:\n",
    "   $$\\log p_X(\\mathbf{x}) = \\log p_Z(f(\\mathbf{x})) + \\log |\\det J_f(\\mathbf{x})|$$\n",
    "\n",
    "3. **Minimize Loss**:\n",
    "   $$\\mathcal{L} = -\\mathbb{E}_{\\mathbf{x}\\sim p_\\text{data}}[\\log p_X(\\mathbf{x})]$$\n",
    "\n",
    "Unlike diffusion models that require multiple denoising steps, RealNVP performs exact likelihood computation in a single pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b3c720-2810-4df5-b86b-43a18d2ae714",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_realnvp(n_steps=10000, batch_size=128, lr=1e-3):\n",
    "    \"\"\"Train the RealNVP model on Swiss Roll data.\"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Initialize model\n",
    "    dim = 2\n",
    "    hidden_dim = 128\n",
    "    num_coupling_layers = 6\n",
    "    model = RealNVP(dim, hidden_dim, num_coupling_layers).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    losses = []\n",
    "    pbar = tqdm(range(n_steps), desc=\"Training RealNVP\")\n",
    "    \n",
    "    for step in pbar:\n",
    "        # Get batch of Swiss Roll data\n",
    "        x = generate_swiss_roll(batch_size).to(device)\n",
    "        \n",
    "        # Forward pass to get latent z\n",
    "        z, log_det_J = model(x)\n",
    "        \n",
    "        # Compute log likelihood\n",
    "        log_pz = -0.5 * (z**2 + np.log(2 * np.pi)).sum(dim=1)\n",
    "        log_px = log_pz + log_det_J\n",
    "        loss = -log_px.mean()\n",
    "        \n",
    "        # Optimization step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "        pbar.set_postfix({'loss': loss.item()})\n",
    "        \n",
    "    return model, losses, device\n",
    "\n",
    "# Train the model\n",
    "model, losses, device = train_realnvp()\n",
    "\n",
    "# Plot training loss\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce78f9c-d52f-43c8-b63d-924bb6a4d51f",
   "metadata": {},
   "source": [
    "## Generation and Visualization\n",
    "\n",
    "To generate samples from the trained RealNVP model, we:\n",
    "\n",
    "1. **Sample from Base Distribution**:\n",
    "   $$\\mathbf{z} \\sim \\mathcal{N}(0, I)$$\n",
    "\n",
    "2. **Apply Inverse Transform**:\n",
    "   $$\\mathbf{x} = f^{-1}(\\mathbf{z})$$\n",
    "\n",
    "The inverse transform is exact and computationally efficient, unlike diffusion models that require multiple steps.\n",
    "\n",
    "**Quality Metrics to Consider:**\n",
    "1. **Coverage**: Does the model capture the full Swiss Roll shape?\n",
    "2. **Density**: Are samples distributed similarly to the training data?\n",
    "3. **Smoothness**: Is the learned transformation continuous?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228daf96-8c12-4a99-9c43-a6140fe6b5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_samples(model, n_samples=1000, device='cpu'):\n",
    "    \"\"\"Generate and visualize samples from the RealNVP model.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        # Sample from base distribution\n",
    "        z = torch.randn(n_samples, 2).to(device)\n",
    "        # Transform to data space\n",
    "        x_gen, _ = model.inverse(z)\n",
    "        x_gen = x_gen.cpu()\n",
    "        \n",
    "    # Get real data for comparison\n",
    "    x_real = generate_swiss_roll(n_samples)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.scatter(x_real[:, 0], x_real[:, 1], \n",
    "                c='blue', alpha=0.5, label='Real')\n",
    "    plt.xlim([-1,1])\n",
    "    plt.ylim([-1,1])\n",
    "    plt.title('Real Swiss Roll')\n",
    "    plt.legend()\n",
    "    # plt.axis('equal')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.scatter(x_gen[:, 0], x_gen[:, 1], \n",
    "                c='red', alpha=0.5, label='Generated')\n",
    "    plt.title('RealNVP Generated')\n",
    "    plt.xlim(-1,1)\n",
    "    plt.ylim(-1,1)\n",
    "    plt.legend()\n",
    "    # plt.axis('equal')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize samples\n",
    "visualize_samples(model, device=device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
