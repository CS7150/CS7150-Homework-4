{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed69ad65-073c-4265-8bf2-b99d3d07bf5f",
   "metadata": {},
   "source": [
    "# MNIST Diffusion Model\n",
    "\n",
    "In this section, we apply the same diffusion model framework to the MNIST dataset. Since MNIST consists \n",
    "of images, we use convolutional architectures (and residual blocks) for improved performance.\n",
    "\n",
    "**Key Points:**\n",
    "- **Architecture:** We use ConvNets with residual connections and a time embedding.\n",
    "- **Data Loading:** The `generate_mnist` function provides a batch of MNIST images normalized to [-1, 1].\n",
    "- **Your Task:** Carefully study the architecture and complete the training, reverse process, and sample generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edeec0a-4bbd-4608-8430-20d82707fa1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "import math\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f325dc9-63b9-48a6-9965-a80bf682cd0b",
   "metadata": {},
   "source": [
    "\n",
    "## Data Generation for MNIST\n",
    "\n",
    "This function downloads MNIST (if not present) and returns a batch of MNIST images with proper normalization:\n",
    "- **Normalization:** We normalize to [-1, 1] so that the diffusion noise scales work similarly to our previous examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ec26e6-e5cc-4795-a574-7b6f2cb08156",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mnist(n_samples=1000):\n",
    "    \"\"\"\n",
    "    Generate MNIST samples, matching the interface of generate_swiss_roll.\n",
    "    \n",
    "    Args:\n",
    "        n_samples: Number of samples to generate.\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor of shape (n_samples, 1, 28, 28)\n",
    "    \"\"\"\n",
    "    # Define transformation: convert to tensor and normalize to [-1, 1]\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "    \n",
    "    # Download MNIST dataset if needed\n",
    "    dataset = torchvision.datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "    \n",
    "    # Create a dataloader to sample n_samples randomly\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=n_samples,\n",
    "        shuffle=True,\n",
    "        drop_last=True\n",
    "    )\n",
    "    \n",
    "    # Get a single batch of images and ignore the labels\n",
    "    images, _ = next(iter(dataloader))\n",
    "    return images\n",
    "\n",
    "# Test the data generator:\n",
    "# sample_images = generate_mnist(16)\n",
    "# print(sample_images.shape)  # Expected: [16, 1, 28, 28]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd16b6d2-d01a-45b3-b123-0c4dbadfeb6c",
   "metadata": {},
   "source": [
    "\n",
    "## Time Embedding\n",
    "\n",
    "We reuse the same time embedding module from the Swiss Roll example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bd7c25-2cb6-4b73-893b-f76ab5fb9d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeEmbedding(nn.Module):\n",
    "    \"\"\"Time embedding layer: embeds scalar timesteps into a higher-dimensional space.\"\"\"\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Sequential(\n",
    "            nn.Linear(1, dim),  # Transform from 1 to hidden dimension.\n",
    "            nn.GELU(),\n",
    "            nn.Linear(dim, dim)  # Map into final embedding size.\n",
    "        )\n",
    "        \n",
    "    def forward(self, t):\n",
    "        t = t.unsqueeze(-1).float()  # Reshape (batch,) -> (batch, 1)\n",
    "        return self.embed(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb78c7d4-b161-4afb-a734-261197752bdf",
   "metadata": {},
   "source": [
    "\n",
    "## Residual Block for ConvNets\n",
    "\n",
    "This block uses two convolutional layers with BatchNorm and GELU activation. Residual connections (and \n",
    "proper scaling) are added optionally.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c46dc08-1263-4c3b-94e2-070af9cdd3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    \"\"\"Residual block with Conv layers for the MNIST diffusion model.\"\"\"\n",
    "    def __init__(self, in_c, out_c, is_res=False):\n",
    "        super().__init__()\n",
    "        self.is_res = is_res\n",
    "        self.same_c = in_c == out_c\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_c, out_c, kernel_size=3, padding=1),  # Convolution layer\n",
    "            nn.BatchNorm2d(out_c),\n",
    "            nn.GELU()\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(out_c, out_c, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_c),\n",
    "            nn.GELU()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x1 = self.conv1(x)  # First conv block\n",
    "        x2 = self.conv2(x1) # Second conv block\n",
    "        if self.is_res:\n",
    "            # Use a residual connection\n",
    "            if self.same_c:\n",
    "                out = x + x2\n",
    "            else:\n",
    "                out = x1 + x2\n",
    "            return out / math.sqrt(2)  # Scale residual output\n",
    "        return x2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9426edeb-ad13-494c-a02e-2dca3d6d8912",
   "metadata": {},
   "source": [
    "## Diffusion Model with ConvNets for MNIST\n",
    "\n",
    "The DiffusionModel below is updated to use convolutional layers, downsampling (via strided convolutions), \n",
    "and upsampling (using transposed convolutions) along with skip-connections.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff80e624-38f9-4c6d-a4b7-67d9d66ec6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffusionModel(nn.Module):\n",
    "    \"\"\"Diffusion model for MNIST with ConvNets, residual blocks, and time embedding.\"\"\"\n",
    "    def __init__(self, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.time_embed = TimeEmbedding(hidden_dim)\n",
    "        \n",
    "        # Initial convolution transforms the input image channels into hidden_dim channels.\n",
    "        self.input_conv = ResBlock(in_c=1, out_c=hidden_dim, is_res=True)\n",
    "        \n",
    "        # Downsampling layers (using convolutions with stride 2)\n",
    "        self.down1 = nn.Sequential(\n",
    "            ResBlock(hidden_dim, hidden_dim),\n",
    "            nn.Conv2d(hidden_dim, hidden_dim, 3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(hidden_dim),\n",
    "            nn.GELU()\n",
    "        )\n",
    "        \n",
    "        self.down2 = nn.Sequential(\n",
    "            ResBlock(hidden_dim, hidden_dim),\n",
    "            nn.Conv2d(hidden_dim, hidden_dim, 3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(hidden_dim),\n",
    "            nn.GELU()\n",
    "        )\n",
    "        \n",
    "        # Bottleneck residual block\n",
    "        self.bottleneck = ResBlock(hidden_dim, hidden_dim, is_res=True)\n",
    "        \n",
    "        # Upsampling with transpose convolutions and skip connections\n",
    "        self.up2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(hidden_dim * 2, hidden_dim, 4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(hidden_dim),\n",
    "            nn.GELU(),\n",
    "            ResBlock(hidden_dim, hidden_dim, is_res=True)\n",
    "        )\n",
    "        \n",
    "        self.up1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(hidden_dim * 2, hidden_dim, 4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(hidden_dim),\n",
    "            nn.GELU(),\n",
    "            ResBlock(hidden_dim, hidden_dim, is_res=True)\n",
    "        )\n",
    "        \n",
    "        # Final convolution to map features back to one channel.\n",
    "        self.output = nn.Conv2d(hidden_dim * 2, 1, 3, padding=1)\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        # Compute time embedding and reshape to add spatial dimensions.\n",
    "        t_emb = self.time_embed(t)\n",
    "        t_emb = t_emb.view(-1, t_emb.shape[1], 1, 1)\n",
    "        \n",
    "        # Initial convolution with skip connection\n",
    "        x1 = self.input_conv(x)\n",
    "        x1 = x1 + t_emb  # Add time information\n",
    "        \n",
    "        # Downsample with added time embedding at each stage.\n",
    "        x2 = self.down1(x1)\n",
    "        x2 = x2 + t_emb\n",
    "        x3 = self.down2(x2)\n",
    "        x3 = x3 + t_emb\n",
    "        \n",
    "        # Bottleneck transformation\n",
    "        x3 = self.bottleneck(x3)\n",
    "        \n",
    "        # Upsample, concatenating skip connections.\n",
    "        x = self.up2(torch.cat([x3, x3], dim=1))  # Using a dummy skip here\n",
    "        x = self.up1(torch.cat([x, x2], dim=1))\n",
    "        \n",
    "        # Final output using skip connection from input\n",
    "        x = self.output(torch.cat([x, x1], dim=1))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb369c6-f698-42f5-a108-fdd58bbdc76c",
   "metadata": {},
   "source": [
    "\n",
    "## Diffusion Scheduler\n",
    "\n",
    "This scheduler is identical to our previous example. It defines the forward (diffuse) and reverse processes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b9b048-c190-489a-8522-f8c34f09d53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffusionScheduler:\n",
    "    \"\"\"Noise scheduler for the diffusion model.\"\"\"\n",
    "    def __init__(self, num_timesteps=1000, device='cpu'):\n",
    "        self.num_timesteps = num_timesteps\n",
    "        self.device = device\n",
    "        \n",
    "        # Define a beta schedule linearly spaced between 1e-4 and 0.02\n",
    "        self.betas = torch.linspace(1e-4, 0.02, num_timesteps).to(device)\n",
    "        self.alphas = 1. - self.betas\n",
    "        self.alphas_cumprod = torch.cumprod(self.alphas, dim=0)\n",
    "        \n",
    "        # Precompute useful square roots for the forward process.\n",
    "        self.sqrt_alphas_cumprod = torch.sqrt(self.alphas_cumprod)\n",
    "        self.sqrt_one_minus_alphas_cumprod = torch.sqrt(1. - self.alphas_cumprod)\n",
    "\n",
    "        # Additional parameters for the reverse process.\n",
    "        self.sqrt_recip_alphas = torch.sqrt(1.0 / self.alphas)\n",
    "        alphas_cumprod_prev = torch.cat([torch.ones(1).to(device), self.alphas_cumprod[:-1]])\n",
    "        self.posterior_variance = self.betas * (1. - alphas_cumprod_prev) / (1. - self.alphas_cumprod)\n",
    "    \n",
    "    def diffuse(self, x_0, t):\n",
    "        \"\"\"Add noise to the data at timestep t.\"\"\"\n",
    "        noise = torch.randn_like(x_0)\n",
    "        # Reshape scales to match image dimensions\n",
    "        sqrt_alpha_t = self.sqrt_alphas_cumprod[t].view(-1, 1, 1, 1)\n",
    "        sqrt_one_minus_alpha_t = self.sqrt_one_minus_alphas_cumprod[t].view(-1, 1, 1, 1)\n",
    "        x_t = sqrt_alpha_t * x_0 + sqrt_one_minus_alpha_t * noise\n",
    "        return x_t, noise\n",
    "    \n",
    "    def reverse_step(self, x_t, t, predicted_noise):\n",
    "        \"\"\"Single reverse process step using the provided formula.\"\"\"\n",
    "        alpha = self.alphas[t]\n",
    "        alpha_bar = self.alphas_cumprod[t]\n",
    "        if t > 0:\n",
    "            noise = torch.randn_like(x_t)\n",
    "        else:\n",
    "            noise = 0\n",
    "        x_t_minus_1 = (1 / torch.sqrt(alpha)) * (\n",
    "            x_t - (1 - alpha) / torch.sqrt(1 - alpha_bar) * predicted_noise\n",
    "        ) + torch.sqrt(self.betas[t]) * noise\n",
    "        return x_t_minus_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fd7853-d4a9-4d46-af6d-89a66e132b02",
   "metadata": {},
   "source": [
    "## Training the MNIST Diffusion Model\n",
    "\n",
    "In this cell, we define the training loop for the MNIST diffusion model. Notice that the training \n",
    "procedure is very similar to the Swiss Roll example. The key differences are:\n",
    "- **Model Architecture:** We use the ConvNet-based DiffusionModel.\n",
    "- **Data:** We work with 28×28 images which have one channel.\n",
    "\n",
    "**Your Task:**  \n",
    "Try to understand the provided code. You are also encouraged to experiment with additional improvements \n",
    "(e.g., using a learning rate scheduler, customizing loss functions, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ca618b-92fc-40c9-99f8-20ddb58f152b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_diffusion(n_steps=10000, batch_size=128, lr=1e-3):\n",
    "    \"\"\"Train the diffusion model on MNIST data.\"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Initialize MNIST diffusion model with convolutional architecture.\n",
    "    model = DiffusionModel().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    # Initialize noise scheduler.\n",
    "    scheduler = DiffusionScheduler(device=device)\n",
    "    \n",
    "    losses = []\n",
    "    pbar = tqdm(range(n_steps), desc=\"Training Diffusion\")\n",
    "    \n",
    "    for step in pbar:\n",
    "        # Get a batch of MNIST images.\n",
    "        x_0 = generate_mnist(batch_size).to(device)\n",
    "        # Sample random timesteps for the batch.\n",
    "        t = torch.randint(0, scheduler.num_timesteps, (x_0.shape[0],)).to(device)\n",
    "        # Apply forward process: add noise.\n",
    "        x_t, noise = scheduler.diffuse(x_0, t)\n",
    "        # Predict the noise using the model.\n",
    "        predicted_noise = model(x_t, t.float() / scheduler.num_timesteps)\n",
    "        # Compute loss (MSE between the predicted noise and the actual noise).\n",
    "        loss = torch.mean((predicted_noise - noise) ** 2)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "        pbar.set_postfix({'loss': loss.item()})\n",
    "    \n",
    "    return model, losses, scheduler, device\n",
    "\n",
    "# Train with a modest number of steps for demonstration:\n",
    "model, losses, scheduler, device = train_diffusion(n_steps=2000)\n",
    "\n",
    "## Visualizing Training Loss\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56dc03b9-a717-48c4-9f3d-ec162a64b73b",
   "metadata": {},
   "source": [
    "## Generating Samples from the Trained Model\n",
    "\n",
    "We now generate MNIST samples from noise and visualize them in a grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725b1c6e-9091-4993-a408-5bdfc94cfe99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_samples(model, scheduler, n_samples=16):\n",
    "    \"\"\"Generate MNIST samples from noise using the reverse process.\"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "    model.eval()\n",
    "    \n",
    "    # Start from random noise.\n",
    "    x = torch.randn(n_samples, 1, 28, 28).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for step in tqdm(range(scheduler.num_timesteps-1, -1, -1), desc=\"Sampling\"):\n",
    "            t = torch.full((n_samples,), step, device=device, dtype=torch.long)\n",
    "            predicted_noise = model(x, t.float() / scheduler.num_timesteps)\n",
    "            x = scheduler.reverse_step(x, step, predicted_noise)\n",
    "    \n",
    "    # Denormalize images from [-1, 1] to [0, 1].\n",
    "    x = (x + 1) / 2\n",
    "    return x.cpu()\n",
    "\n",
    "def visualize_samples(samples, nrow=4):\n",
    "    \"\"\"Visualize generated samples in a grid.\"\"\"\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    for i in range(len(samples)):\n",
    "        plt.subplot(nrow, nrow, i+1)\n",
    "        plt.imshow(samples[i, 0], cmap='gray')\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Generate and display samples.\n",
    "samples = generate_samples(model, scheduler)\n",
    "visualize_samples(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da6f0bf-0216-48fb-9d9b-bcc3e130f8a5",
   "metadata": {},
   "source": [
    "\n",
    "## Visualizing Forward & Reverse Processes\n",
    "\n",
    "The following functions help you visualize:\n",
    "- **Forward Process:** How images get noisier.\n",
    "- **Reverse Process:** How noise is gradually denoised into images.\n",
    "\n",
    "Note: In the forward process, we use `generate_mnist` (instead of an undefined `load_mnist`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c72c5c-7560-4ccf-aec2-7aa7e5a591f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_forward_process(model, scheduler, n_samples=16):\n",
    "    \"\"\"\n",
    "    Visualize how MNIST digits get progressively noisier in the forward process.\n",
    "    \"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    # Generate initial clean images.\n",
    "    x_0 = generate_mnist(n_samples).to(device)\n",
    "    \n",
    "    max_timestep = scheduler.num_timesteps - 1\n",
    "    plot_steps = [int(t) for t in [0, max_timestep*0.02, max_timestep*0.05, \n",
    "                                   max_timestep*0.1, max_timestep*0.2, max_timestep*0.5, max_timestep]]\n",
    "    \n",
    "    fig, axes = plt.subplots(1, len(plot_steps), figsize=(15, 3))\n",
    "    \n",
    "    for idx, t in enumerate(plot_steps):\n",
    "        timesteps = torch.full((n_samples,), t, device=device, dtype=torch.long)\n",
    "        x_t, _ = scheduler.diffuse(x_0, timesteps)\n",
    "        \n",
    "        ax = axes[idx]\n",
    "        # Display the first image of the batch at the selected timestep.\n",
    "        img = x_t[0].detach().cpu()\n",
    "        ax.imshow(img[0], cmap='gray')\n",
    "        ax.set_title(f't = {t}')\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.suptitle('Forward Process: Clean Image → Noise', y=1.05)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def visualize_reverse_process(model, scheduler, n_samples=16):\n",
    "    \"\"\"\n",
    "    Visualize the reverse process: the gradual denoising of noise into an MNIST digit.\n",
    "    \"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    x = torch.randn(n_samples, 1, 28, 28).to(device)\n",
    "    \n",
    "    max_timestep = scheduler.num_timesteps - 1\n",
    "    plot_steps = [int(t) for t in [max_timestep, max_timestep*0.5, max_timestep*0.2,\n",
    "                                   max_timestep*0.1, max_timestep*0.05, max_timestep*0.02, 0]]\n",
    "    \n",
    "    fig, axes = plt.subplots(1, len(plot_steps), figsize=(15, 3))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for step in tqdm(range(scheduler.num_timesteps-1, -1, -1), desc=\"Sampling\"):\n",
    "            t = torch.full((n_samples,), step, device=device, dtype=torch.long)\n",
    "            predicted_noise = model(x, t.float() / scheduler.num_timesteps)\n",
    "            x = scheduler.reverse_step(x, step, predicted_noise)\n",
    "            \n",
    "            if step in plot_steps:\n",
    "                idx = plot_steps.index(step)\n",
    "                ax = axes[idx]\n",
    "                img = x[0].detach().cpu()\n",
    "                ax.imshow(img[0], cmap='gray')\n",
    "                ax.set_title(f't = {step}')\n",
    "                ax.axis('off')\n",
    "    \n",
    "    plt.suptitle('Reverse Process: Noise → MNIST Digit', y=1.05)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize forward process:\n",
    "visualize_forward_process(model, scheduler)\n",
    "\n",
    "# Visualize reverse process:\n",
    "visualize_reverse_process(model, scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bd3ccb-6e65-44b7-b9c1-55546c3073ad",
   "metadata": {},
   "source": [
    "## Comparing Real and Generated MNIST Samples\n",
    "\n",
    "Display real MNIST digits (from the generator) side-by-side with the generated samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab965385-a1e7-4dc6-b1d1-9938442a6647",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot real MNIST data.\n",
    "plt.subplot(1, 2, 1)\n",
    "real_data = generate_mnist(16)  # Fetch 16 real MNIST images.\n",
    "plt.title('Real MNIST')\n",
    "grid_size = int(np.sqrt(16))  # 4x4 grid.\n",
    "for i in range(16):\n",
    "    row = i // grid_size\n",
    "    col = i % grid_size\n",
    "    plt.subplot2grid((4, 8), (row, col))\n",
    "    plt.imshow(real_data[i, 0], cmap='gray')\n",
    "    plt.axis('off')\n",
    "\n",
    "# Plot generated MNIST samples.\n",
    "plt.subplot(1, 2, 2)\n",
    "samples = generate_samples(model, scheduler, 16)\n",
    "plt.title('Diffusion Generated')\n",
    "for i in range(16):\n",
    "    row = i // grid_size\n",
    "    col = i % grid_size\n",
    "    plt.subplot2grid((4, 8), (row, col + 4))  # Offset columns for side-by-side view.\n",
    "    plt.imshow(samples[i, 0], cmap='gray')\n",
    "    plt.axis('off')\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a32cc7-6c5e-4ef1-ba4e-3275b07ffecb",
   "metadata": {},
   "source": [
    "## Your `To-Do` \n",
    " - Can you get the generated numbers closer to the real data?\n",
    " - Hint: Try increasing the number of timesteps in diffusion process. You can also try increasing the training steps, maybe play with the architecture a bit ...\n",
    "\n",
    "Put your modified code, and images that show your improved samples, in a new cell below.",
    "\n",
    "\n",
    "For extra credit, try adapting a GAN, VAE, Flow matching, or RealNVP to generate MNIST."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
